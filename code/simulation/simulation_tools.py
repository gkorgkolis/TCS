import json

import numpy as np
import pandas as pd
from sklearn.metrics import roc_auc_score
from tqdm import trange
from utils import _from_full_to_cp

from simulation.simulation_configs import cd_config as CD_CONFIGS
from simulation.simulation_configs import noise_config as NOISE_CONFIGS
from simulation.simulation_configs import pred_config as PRED_CONFIGS
from simulation.simulation_metrics import (prepare_det_data,
                                           run_detection_metrics,
                                           run_detection_metrics_XY)
from simulation.simulation_utils import simulate


def get_optimal_sim(
        true_data: pd.DataFrame, 
        CONFIGS: dict = None, 
        done_eval: bool = False,
        optimal_det_config: dict = None,
        optimal_det_func: dict = None, 
        verbose: bool = True
) -> dict:
    """ 
    Runs the simulation process for several combinations of parameter values, then returns the optimal configuration, 
    optimized over the optimal detection score, where the detector is in turn optimized over a selected hyper-parameter 
    search space.

    Args
    ----
    true_data (pd.DataFrame) : The real data
    CONFIGS (dict) : Dictionary containing the search spaces for the optimal Simulation configuration (default = None)
    done_eval (bool) : Skips the fine-tuning process of the evaluators (lstm-based and svm-based C2ST); 
                    if `True`, then `optimal_det_config` & `optimal_det_func` should be provided (default = `False`) 
    optimal_det_config (dict) : Optimal configuration for the optimal C2ST (default = `None`)
    optimal_det_func (callable) : Optimal C2ST
    sparsity_penalty (bool) : If `True`, chooses from the top-k configurations (according to their detection auc) the one 
                    with the most sparse structure, given that its scored AUC is below a threshold of `0.7`. Otherwise, the  
                    in order configuration is picked instead. If no such configuration exists, the returned configuration 
                    is the one with the optimal detection AUC, no matter the structure density (default = `True`)
    sparsity_k (int) : Number of top configurations checked by the sparsity penalty (default = `5`)
    sparsity_diff (float) : the AUC difference accepted by the sparsity penalty (default = `0.05`) 
    verbose (bool) : Whether or not to print descriptive log messages during the process (default = `True`)

    Returns
    ------
    res (dict) : Dictionary with the following fields:
        - optimal_data (pd.DataFrame) : Simulated data generated by the optimal configuration
        - optimal_config (dict) : The optimal Simulation configuration
        - optimal_detector (callable) : The function of the optimal detector
        - optimal_det_config (dict) : The optimal configuration for the optimal detector
        - auc (dict) - the auc
    """

    if CONFIGS is None:
        CONFIGS = [
            {
                "cd": cd_method, 
                "fc": fc_method,
                "z": z_approximation,
                "o": z_approximation
            } for cd_method in list(CD_CONFIGS.values()) 
            for fc_method in list(PRED_CONFIGS.values())
            for z_approximation in list(NOISE_CONFIGS.values())
        ]

    if verbose:
        print(f"LOG: Optimal Simulation: {len(CONFIGS)} are to be tested ...")

    simulated_data_list = []
    det_config_list = []
    det_func_list = []
    fit_scm_list = []
    scores_list = []
    labels_list = []
    probs_list = []
    auc_list = []

    for cfg in CONFIGS:

        if verbose:
            print(f"_____________________________________________________________________________________________________________")
            print(f"_______________________ {cfg} _______________________")
        
        cd_method = cfg['cd']["cd_method"]
        cd_kwargs = cfg['cd']["cd_kwargs"]
        pred_method = cfg['fc']["pred_method"]
        pred_kwargs = cfg['fc']["pred_kwargs"]

        try:

            simulated_data, fit_scm, funcs_and_noise, scores = simulate(
                    true_data=true_data, 
                    true_label=None,
                    cd_method=cd_method, 
                    cd_kwargs=cd_kwargs, 
                    pred_method=pred_method, 
                    pred_kwargs=pred_kwargs,
                    o_approximation=cfg['o'],
                    noise_approximation=cfg['z'],
                    n_samples=len(true_data),
                    verbose=False
                )
            simulated_data_list.append(simulated_data)
            fit_scm_list.append(fit_scm)
            scores_list.append(scores)

            if not done_eval:
                # reduced the # samples used during the seach for the optimal detector - approximation
                # res = run_detection_metrics(real=true_data[:min([500, len(true_data)])], 
                #                             synthetic=simulated_data[:min([500, len(true_data)])])
                res = run_detection_metrics(real=true_data, synthetic=simulated_data, verbose=False)
                optimal_det_config = res["config"]
                optimal_det_func = res["detector"]
                done_eval = True
            else:
                # res = run_detection_metrics(real=true_data[:min([500, len(true_data)])], 
                #                             synthetic=simulated_data[:min([500, len(true_data)])])
                res = run_detection_metrics(real=true_data, synthetic=simulated_data, verbose=False)
                optimal_det_config = res["config"]
                optimal_det_func = res["detector"]
            
            auc, probs, labels = optimal_det_func(real=true_data, synthetic=simulated_data, **optimal_det_config)
            auc_list.append(auc)
            probs_list.append(probs)
            labels_list.append(labels)
            det_func_list.append(optimal_det_func)
            det_config_list.append(optimal_det_config)

            if verbose:
                print(f"LOG: Optimal Simulation: Max C2ST AUC achieved: {auc}")
        
        except:
            CONFIGS.remove(cfg)
            if verbose:
                print(f"LOG: Optimal Simulation: A configuration failed to run, thus is removed : {len(CONFIGS) + 1} -> {len(CONFIGS)}")
            continue
    
        if verbose:
            print(f"_____________________________________________________________________________________________________________")

        idx = np.argmin([np.abs(0.5-x) for x in auc_list])

    if verbose:
        print(f"LOG: Optimal Simulation ({auc_list[idx]} auc): chosen configuration: {json.dumps(CONFIGS[idx], sort_keys=True, indent=4)}")

    res =  {
        "optimal_data" : simulated_data_list[idx],
        "optimal_scm" : fit_scm_list[idx],
        "optimal_config": CONFIGS[idx], 
        "optimal_detector" : det_func_list[idx], 
        "optimal_detector_config": det_config_list[idx], 
        "scores": scores_list[idx],
        "auc": auc_list[idx]
    }

    return res


def equivalence_test(
    auc_o : float, 
    probs_o : list,
    labels_o : list,
    auc_i : float, 
    probs_i : list,
    labels_i : list, 
    a : float = 0.05,
    n : int = 100
):
    """ 
    Performs the equivalence test between two AUC values. 

    Args
    ----
    auc_o (float) : AUC of the first model
    probs_o (list) : Predicted probabilities of the first model
    labels_o (list) : True labels of the first model
    auc_i (float) : AUC of the second model
    probs_i (list) : Predicted probabilities of the second model
    labels_i (list) : True labels of the second model
    a (float) : Significance level (default: `0.05`)
    n (int) : Number of permutations (default: `100`)

    Returns
    ------
    p_value (float) : p-value of the equivalence test
    """
    # adjust arguments
    mil = min([len(labels_o), len(labels_i)])
    probs_o = probs_o[:mil]
    labels_o = labels_o[:mil]
    probs_i = probs_i[:mil]
    labels_i = labels_i[:mil]

    # placeholders
    ctr = 0

    # observed difference
    t_obs = np.abs(auc_o - auc_i)

    # permutation loop 
    for _ in trange(n):
        # permute (swap)
        permute_ind = np.random.choice(a=np.arange(probs_o.shape[0]), size=[probs_o.shape[0]//2], replace=False)
        probs_po = probs_o.copy()
        probs_pi = probs_i.copy()
        probs_pi[permute_ind] = probs_o[permute_ind]
        probs_po[permute_ind] = probs_i[permute_ind]

        # recompute aucs & their difference
        auc_po = roc_auc_score(y_true=labels_o, y_score=probs_po)
        auc_pi = roc_auc_score(y_true=labels_i, y_score=probs_pi)

        # calculate the statistic
        t_mod = np.abs(auc_po - auc_pi)

        if t_mod >= t_obs:
            ctr += 1

    pval = ctr/n
    return True if pval > a else False


def get_optimal_sim_XY(
        true_data: pd.DataFrame, 
        CONFIGS: dict = None, 
        done_eval: bool = False,
        optimal_det_config: dict = None,
        optimal_det_func: dict = None, 
        sparsity_penalty: bool = False,
        verbose: bool = True
) -> dict:
    """ 
    Runs the simulation process for several combinations of parameter values, then returns the optimal configuration, 
    optimized over the optimal detection score, where the detector is in turn optimized over a selected hyper-parameter 
    search space.

    Args
    ----
    true_data (pandas.DataFrame) : the real data
    CONFIGS (dict) : Dictionary containing the search spaces for the optimal simulation configuration (default = `None`)
    done_eval (bool) : Skips the fine-tuning process of the evaluators (lstm-based and svm-based C2ST); 
                    if `True`, then `optimal_det_config` & `optimal_det_func` should be provided (default = `False`) 
    optimal_det_config (dict) : The optimal configuration for the optimal C2ST (default = `None`)
    optimal_det_func (callable) : The optimal C2ST
    sparsity_penalty (bool) : If `True`, it performs a permutation-based statistical test on the discriminator's predicted 
                    probabilities (details in `TCS ref` ), to find configurations with AUC statistical equivalent to the 
                    current optimal; the returned configuration is the sparsest one between those that are found equivalent. 
                    If no such configuration exists, the returned configuration is the one with the optimal detection auc, 
                    no matter the structure density (default = `True`)
    verbose (bool) : Whether or not to print descriptive log messages during the process (default = `True`)

    Returns
    ----
    res (dict) : Dictionary with the following fields:
        - optimal_data (pd.DataFrame) : Simulated data generated by the optimal configuration
        - optimal_config (dict) : Optimal Simulation configuration
        - optimal_detector (callable) : Thefunction of the optimal detector
        - optimal_det_config (dict) : The optimal configuration for the optimal detector
        - auc (dict) - the AUC
    """

    if CONFIGS is None:
        CONFIGS = [
            {
                "cd": cd_method, 
                "fc": fc_method,
                "z": z_approximation,
                "o": z_approximation
            } for cd_method in list(CD_CONFIGS.values()) 
            for fc_method in list(PRED_CONFIGS.values())
            for z_approximation in list(NOISE_CONFIGS.values())
        ]

    if verbose:
        print(f"LOG: Optimal Simulation: {len(CONFIGS)} TCS configurations are to be tested ...")

    simulated_data_list = []
    det_config_list = []
    det_func_list = []
    fit_scm_list = []
    scores_list = []
    labels_list = []
    probs_list = []
    auc_list = []

    for cfg in CONFIGS:

        if verbose:
            print(f"_____________________________________________________________________________________________________________")
            print(f"_______________________ {cfg} _______________________")
        
        cd_method = cfg['cd']["cd_method"]
        cd_kwargs = cfg['cd']["cd_kwargs"]
        pred_method = cfg['fc']["pred_method"]
        pred_kwargs = cfg['fc']["pred_kwargs"]

        try:

            simulated_data, fit_scm, funcs_and_noise, scores = simulate(
                    true_data=true_data, 
                    true_label=None,
                    cd_method=cd_method, 
                    cd_kwargs=cd_kwargs, 
                    pred_method=pred_method, 
                    pred_kwargs=pred_kwargs,
                    o_approximation=cfg['o'],
                    noise_approximation=cfg['z'],
                    n_samples=len(true_data),
                    verbose=False
                )
            simulated_data_list.append(simulated_data)
            fit_scm_list.append(fit_scm)
            scores_list.append(scores)

            train_X, train_Y, test_X, test_Y = prepare_det_data(real=true_data, synthetic=simulated_data)

            if not done_eval:
                res = run_detection_metrics_XY(train_X=train_X, train_Y=train_Y, test_X=test_X, test_Y=test_Y, verbose=False)
                optimal_det_config = res["config"]
                optimal_det_func = res["detector"]
                done_eval = True
            else:
                res = run_detection_metrics_XY(train_X=train_X, train_Y=train_Y, test_X=test_X, test_Y=test_Y, verbose=False)
                optimal_det_config = res["config"]
                optimal_det_func = res["detector"]
            
            auc, probs, labels = optimal_det_func(train_X, train_Y, test_X, test_Y, **optimal_det_config)
            auc_list.append(auc)
            probs_list.append(probs)
            labels_list.append(labels)
            det_func_list.append(optimal_det_func)
            det_config_list.append(optimal_det_config)

            if verbose:
                print(f"LOG: Optimal Simulation: Max C2ST AUC achieved: {auc}")
        
        except:
            CONFIGS.remove(cfg)
            if verbose:
                print(f"LOG: Optimal Simulation: A configuration failed to run, thus is removed : {len(CONFIGS) + 1} -> {len(CONFIGS)}")
            continue
    
        if verbose:
            print(f"_____________________________________________________________________________________________________________")

    # sparsity penalty
    if sparsity_penalty:
        if verbose:
            print(f"LOG: Optimal Simulation: Enforcing sparcity penalty ...")
        # derive the cp-adjacency matrix
        cp_list = [_from_full_to_cp(x).sum().item() if isinstance(x, pd.DataFrame) 
                        else x.causal_structure.causal_structure_cp.sum().item() for x in fit_scm_list]
        # list of indices with optimal performance
        idx = np.argmin([np.abs(0.5-x) for x in auc_list])
        eq_list = [idx]
        print(f"LOG: Optimal Simulation: Sparsity Penalty: optimal case: idx={idx} | auc={auc_list[idx]} | edges={cp_list[idx]}")
        # permutation loop
        for i, _ in enumerate(auc_list): 
            if i!=idx:
                dcs = equivalence_test(
                    auc_o=auc_list[idx], 
                    probs_o=probs_list[idx],
                    labels_o=labels_list[idx],
                    auc_i=auc_list[i], 
                    probs_i=probs_list[i],
                    labels_i=labels_list[i],
                    a=0.05,
                    n=100
                )
                if dcs:
                    eq_list.append(i)
                    print(f"LOG: Optimal Simulation: Sparsity Penalty: eq. found: idx={i} | auc={auc_list[i]} | edges={cp_list[i]}")
        eq_cp = [(i, cp_list[i]) for i in eq_list]
        idx = sorted(eq_cp, key=lambda x: x[1])[0][0]
        print(f"LOG: Optimal Simulation: Sparsity Penalty: opti case: idx={idx} | auc={auc_list[idx]} | edges={cp_list[idx]}")
    else:
        idx = np.argmin([np.abs(0.5-x) for x in auc_list])

    if verbose:
        print(f"LOG: Optimal Simulation ({auc_list[idx]} auc): chosen configuration: {json.dumps(CONFIGS[idx], sort_keys=True, indent=4)}")

    res =  {
        "optimal_data" : simulated_data_list[idx],
        "optimal_scm" : fit_scm_list[idx],
        "optimal_config": CONFIGS[idx], 
        "optimal_detector" : det_func_list[idx], 
        "optimal_detector_config": det_config_list[idx], 
        "scores": scores_list[idx],
        "auc": auc_list[idx]
    }

    return res


# NOTE: specific to experiments inspecting the behavior of sparsity penalty
def get_optimal_sim_XY_dual(
        true_data: pd.DataFrame, 
        CONFIGS: dict = None, 
        done_eval: bool = False,
        optimal_det_config: dict = None,
        optimal_det_func: dict = None, 
        sparsity_penalty: bool = False,
        verbose: bool = True
) -> dict:
    """ 
    Runs the simulation process for several combinations of parameter values, then returns the optimal configuration, 
    optimized over the optimal detection score, where the detector is in turn optimized over a selected hyper-parameter 
    search space.

    Args
    ----
    true_data (pandas.DataFrame) : Real data
    CONFIGS (dict) : Dictionary containing the search spaces for the optimal Simulation configuration (default = `None`)
    done_eval (bool) : skips the fine-tuning process of the evaluators (lstm-based and svm-based C2ST); 
                    if `True`, then `optimal_det_config` & `optimal_det_func` should be provided (default = `False`) 
    optimal_det_config (dict) : The optimal configuration for the optimal C2ST (default = `None`)
    optimal_det_func (callable) : The optimal C2ST
    sparsity_penalty (bool) : If `True`, chooses from the top-k configurations (according to their detection AUC) the one 
                    with the most sparse structure, given that its scored AUC is below a threshold of `0.7`. Otherwise, the  
                    in-order configuration is picked instead. If no such configuration exists, the returned configuration 
                    is the one with the optimal detection AUC, no matter the structure density (default = `True`)
    sparsity_k (int) : Number of top configurations checked by the sparsity penalty (default = `5`)
    sparsity_diff (float) : Absolute AUC difference accepted by the sparsity penalty (default = `0.05`) 
    verbose (bool) : Whether or not to print descriptive log messages during the process (default = `True`)

    Returns
    ----
    res (dict) : Dictionary with the following fields:
        - optimal_data (pandas.DataFrame) : Simulated data generated by the optimal configuration
        - optimal_config (dict) : Optimal Simulation configuration
        - optimal_detector (callable) : Function of the optimal detector
        - optimal_det_config (dict) : Optimal configuration for the optimal detector
        - auc (dict) - The AUC 
    """

    if CONFIGS is None:
        CONFIGS = [
            {
                "cd": cd_method, 
                "fc": fc_method,
                "z": z_approximation,
                "o": z_approximation
            } for cd_method in list(CD_CONFIGS.values()) 
            for fc_method in list(PRED_CONFIGS.values())
            for z_approximation in list(NOISE_CONFIGS.values())
        ]

    if verbose:
        print(f"LOG: Optimal Simulation: {len(CONFIGS)} TCS configurations are to be tested ...")

    simulated_data_list = []
    det_config_list = []
    det_func_list = []
    fit_scm_list = []
    scores_list = []
    labels_list = []
    probs_list = []
    auc_list = []

    for cfg in CONFIGS:

        if verbose:
            print(f"_____________________________________________________________________________________________________________")
            print(f"_______________________ {cfg} _______________________")
        
        cd_method = cfg['cd']["cd_method"]
        cd_kwargs = cfg['cd']["cd_kwargs"]
        pred_method = cfg['fc']["pred_method"]
        pred_kwargs = cfg['fc']["pred_kwargs"]

        try:

            simulated_data, fit_scm, funcs_and_noise, scores = simulate(
                    true_data=true_data, 
                    true_label=None,
                    cd_method=cd_method, 
                    cd_kwargs=cd_kwargs, 
                    pred_method=pred_method, 
                    pred_kwargs=pred_kwargs,
                    o_approximation=cfg['o'],
                    noise_approximation=cfg['z'],
                    n_samples=len(true_data),
                    verbose=False
                )
            simulated_data_list.append(simulated_data)
            fit_scm_list.append(fit_scm)
            scores_list.append(scores)

            train_X, train_Y, test_X, test_Y = prepare_det_data(real=true_data, synthetic=simulated_data)

            if not done_eval:

                res = run_detection_metrics_XY(train_X=train_X, train_Y=train_Y, test_X=test_X, test_Y=test_Y, verbose=False)
                optimal_det_config = res["config"]
                optimal_det_func = res["detector"]
                done_eval = True
            else:
                res = run_detection_metrics_XY(train_X=train_X, train_Y=train_Y, test_X=test_X, test_Y=test_Y, verbose=False)
                optimal_det_config = res["config"]
                optimal_det_func = res["detector"]
            
            auc, probs, labels = optimal_det_func(train_X, train_Y, test_X, test_Y, **optimal_det_config)
            auc_list.append(auc)
            probs_list.append(probs)
            labels_list.append(labels)
            det_func_list.append(optimal_det_func)
            det_config_list.append(optimal_det_config)

            if verbose:
                print(f"LOG: Optimal Simulation: Max C2ST AUC achieved: {auc}")
        
        except:
            CONFIGS.remove(cfg)
            if verbose:
                print(f"LOG: Optimal Simulation: A configuration failed to run, thus is removed : {len(CONFIGS) + 1} -> {len(CONFIGS)}")
            continue
    
        if verbose:
            print(f"_____________________________________________________________________________________________________________")

    # sparsity penalty 
    cp_list = [_from_full_to_cp(x).sum().item() if isinstance(x, pd.DataFrame) 
                    else x.causal_structure.causal_structure_cp.sum().item() for x in fit_scm_list]
    # list of indices with optimal performance
    idx = np.argmin([np.abs(0.5-x) for x in auc_list])
    eq_list = [idx]
    print(f"LOG: Optimal Simulation: Sparsity Penalty: optimal case: idx={idx} | auc={auc_list[idx]} | edges={cp_list[idx]}")
    # permutation loop
    for i, _ in enumerate(auc_list): 
        if i!=idx:
            dcs = equivalence_test(
                auc_o=auc_list[idx], 
                probs_o=probs_list[idx],
                labels_o=labels_list[idx],
                auc_i=auc_list[i], 
                probs_i=probs_list[i],
                labels_i=labels_list[i],
                a=0.05,
                n=100
            )
            if dcs:
                eq_list.append(i)
                print(f"LOG: Optimal Simulation: Sparsity Penalty: eq. found: idx={i} | auc={auc_list[i]} | edges={cp_list[i]}")
    eq_cp = [(i, cp_list[i]) for i in eq_list]
    idx_s = sorted(eq_cp, key=lambda x: x[1])[0][0]
    print(f"LOG: Optimal Simulation: Sparsity Penalty: opti case: idx={idx_s} | auc={auc_list[idx_s]} | edges={cp_list[idx_s]}")

    if verbose:
        print(f"LOG: Optimal Simulation ({auc_list[idx]} auc): chosen configuration: {json.dumps(CONFIGS[idx], sort_keys=True, indent=4)}")

    res = {
        "optimal_data" : simulated_data_list[idx],
        "optimal_scm" : fit_scm_list[idx],
        "optimal_config": CONFIGS[idx], 
        "optimal_detector" : det_func_list[idx], 
        "optimal_detector_config": det_config_list[idx], 
        "scores": scores_list[idx],
        "auc": auc_list[idx]
    }

    res_s = {
        "optimal_data" : simulated_data_list[idx_s],
        "optimal_scm" : fit_scm_list[idx_s],
        "optimal_config": CONFIGS[idx_s], 
        "optimal_detector" : optimal_det_func, 
        "optimal_detector_config": optimal_det_config, 
        "scores": scores_list[idx_s],
        "auc": auc_list[idx_s]
    }

    return res, res_s