{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2192a57",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869e24d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import string\n",
    "import itertools\n",
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from tempogen.temporal_scm import TempSCM\n",
    "from tempogen.functional_utils import (_torch_tanh, _torch_arctan, _torch_sin, _torch_cos,\n",
    "                                       _torch_sigmoid, _torch_pow, _torch_identity, _torch_sqrt)\n",
    "\n",
    "from tempogen.temporal_random_generation import get_p_edge, get_funcs, get_z_distribution \n",
    "from simulation.simulation_utils import simulate\n",
    "\n",
    "from cdt.metrics import SHD\n",
    "from simulation.simulation_metrics import mmd_torch\n",
    "\n",
    "from simulation.simulation_tools import get_optimal_sim_XY, run_detection_metrics_XY, prepare_det_data, run_detection_metrics\n",
    "from utils import custom_binary_metrics, _from_full_to_cp\n",
    "\n",
    "from CausalTime.tools import generate_CT\n",
    "from synthcity.plugins import Plugins\n",
    "from synthcity.plugins.core.dataloader import TimeSeriesDataLoader\n",
    "\n",
    "from sdv.metadata import Metadata\n",
    "from sdv.sequential import PARSynthesizer\n",
    "\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "def prind(di): print(json.dumps(di, sort_keys=False, indent=4))\n",
    "\n",
    "COL_NAMES = list(string.ascii_uppercase) + [\"\".join(a) for a in list(itertools.permutations(list(string.ascii_uppercase), r=2))]\n",
    "\n",
    "par_dir = Path(os.getcwd()).parents[1].as_posix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645f1a05",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ace2dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting function\n",
    "def plot_intervened_ts(data, i_step=600):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    fig = plt.figure(layout=\"constrained\", figsize=(9, 3))\n",
    "    # gs = GridSpec(nrows=2, ncols=2, figure=fig, width_ratios=(2, 1))\n",
    "\n",
    "    gs = GridSpec(nrows=2, ncols=2, figure=fig, width_ratios=(1, i_step/1000))\n",
    "    ax1 = fig.add_subplot(gs[1, :])\n",
    "    ax2 = fig.add_subplot(gs[0, 0])\n",
    "    ax3 = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "    sns.lineplot(data=data, ax=ax1, legend=False, palette=sns.color_palette(\"pastel6\", n_colors=data.shape[1]))\n",
    "    ax1.axvline(x=i_step, color=\"black\", linestyle=\"--\", alpha=0.6)\n",
    "    ax1.set_yticks(ticks=[], labels=[])\n",
    "    ax1.set_xlabel(\"Intervened Time-series\", fontdict={\"size\": 12, \"weight\": \"roman\"})\n",
    "\n",
    "    sns.lineplot(data=data.loc[:int(len(data)*(i_step/1000))], ax=ax2, legend=False, palette=sns.color_palette(\"pastel6\", n_colors=data.shape[1]))\n",
    "    ax2.set_yticks(ticks=[], labels=[])\n",
    "    ax2.set_xlabel(\"Data before intervention\", fontdict={\"size\": 12, \"weight\": \"roman\"})\n",
    "\n",
    "    sns.lineplot(data=data.loc[int(len(data)*(i_step/1000)):], ax=ax3, legend=False, palette=sns.color_palette(\"pastel6\", n_colors=data.shape[1]))\n",
    "    ax3.set_yticks(ticks=[], labels=[])\n",
    "    ax3.set_xlabel(\"Data after intervention\", fontdict={\"size\": 12, \"weight\": \"roman\"})\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# paths\n",
    "par_dir = Path(os.getcwd()).parents[1].as_posix()\n",
    "\n",
    "# (hyper) parameters\n",
    "FN = \"cp_0\"\n",
    "SF = \"L_1L+-\"\n",
    "NS = 500\n",
    "N_SCMS = 20\n",
    "FNS = 1\n",
    "IPR = range(int(0.5*NS), int(0.75*NS), 1)\n",
    "LOC = 5.0\n",
    "SCALE = 1.0\n",
    "\n",
    "# # the space for the # vars during random generation\n",
    "n_vars_space = {\n",
    "    \"a\": np.arange(start=3, stop=13, step=1).tolist(), \n",
    "    \"p\": np.full(fill_value=0.1, shape=[10]).tolist()\n",
    "    # \"p\": [0.05, 0.05, 0.125, 0.125, 0.15, 0.15, 0.125, 0.125, 0.05, 0.05]\n",
    "}\n",
    "# - dynamic version of get_n_vars, able to favor specific size of graphs \n",
    "def get_n_vars(a, p):\n",
    "    return rng.choice(a=a, p=p)\n",
    "\n",
    "# # the space for the # lags during random generation\n",
    "n_lags_space = {\n",
    "    \"a\": [1, 2, 3], \n",
    "    \"p\": [1.0, 0.0, 0.0]\n",
    "}\n",
    "# - dynamic version of get_n_vars, able to favor specific size of graphs \n",
    "def get_n_lags(a, p):\n",
    "    return rng.choice(a=a, p=p)\n",
    "\n",
    "# the space for the edge probability during random generation \n",
    "# - dynamic version, depending on the # vars & # lags, to keep large graphs sparser; based on preconfigured options; still testing this\n",
    "def p_edge_space(n_vars, n_lags):\n",
    "    total_edges = (n_vars**2)*n_lags\n",
    "    if total_edges < 100:\n",
    "        return {\n",
    "            \"c\": None,\n",
    "            \"values\": [3/total_edges, 5/total_edges, 7/total_edges],\n",
    "            \"weights\": [0.6, 0.3, 0.1] \n",
    "        }\n",
    "    elif total_edges < 200:\n",
    "        return {\n",
    "            \"c\": None,\n",
    "            \"values\": [5/total_edges, 7/total_edges, 9/total_edges],\n",
    "            \"weights\": [0.6, 0.3, 0.1] \n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"c\": None,\n",
    "            \"values\": [9/total_edges, 12/total_edges, 15/total_edges],\n",
    "            \"weights\": [0.6, 0.3, 0.1] \n",
    "        }\n",
    "\n",
    "# the space for functional dependencies during random generation\n",
    "funcs_space = {\n",
    "    \"c\": None, \n",
    "    \"functions\": [_torch_identity, _torch_sqrt, _torch_sigmoid, _torch_sin, _torch_cos, _torch_arctan],\n",
    "    # \"weights\": [0.2, 0.2, 0.1, 0.1, 0.1, 0.3],\n",
    "    \"weights\": [0.4, 0.4, 0.2, 0.0, 0.0, 0.0],  \n",
    "}\n",
    "\n",
    "# the space for noise distribution during random generation\n",
    "z_distribution_space = {\n",
    "    \"c\": None,\n",
    "    \"functions\": [\n",
    "    torch.distributions.normal.Normal(loc=0, scale=0.005), \n",
    "    torch.distributions.uniform.Uniform(low=-0.15, high=0.15)],\n",
    "    \"weights\": [0.01, 0.99] \n",
    "}\n",
    "\n",
    "def _torch_noisy_f(x, loc=LOC, scale=SCALE, func=_torch_identity):\n",
    "    \"\"\" \"\"\"\n",
    "    noise_dist = torch.distributions.normal.Normal(loc=loc, scale=scale)\n",
    "    return func(x) + noise_dist.sample()\n",
    "\n",
    "i_step = np.random.choice(IPR)\n",
    "auc_ori_list = []\n",
    "auc_pre_list = []\n",
    "auc_post_list = []\n",
    "\n",
    "# main loop for random SCM generation\n",
    "for ctr in range(N_SCMS):\n",
    "\n",
    "    # DFs & SCMs\n",
    "    n_vars = get_n_vars(**n_vars_space)\n",
    "    n_lags = get_n_lags(**n_lags_space)\n",
    "    p_edge = get_p_edge(**p_edge_space(n_vars=n_vars, n_lags=n_lags))\n",
    "    scm = TempSCM(\n",
    "        method=\"C\",\n",
    "        n_vars=n_vars,\n",
    "        n_lags=n_lags,\n",
    "        p_edge=p_edge,\n",
    "        funcs=get_funcs(**funcs_space),\n",
    "        z_distributions=get_z_distribution(**z_distribution_space)   \n",
    "    )\n",
    "\n",
    "    # Generate obervational\n",
    "    ts_pre = scm.generate_time_series(n_samples=i_step).reset_index(drop=True)\n",
    "    print(f\"len(ts_pre): {len(ts_pre)}\")\n",
    "\n",
    "    # Resample DFs & SCMs in case of infinity values\n",
    "    while np.isinf(ts_pre.values.sum()):\n",
    "        n_vars = get_n_vars(**n_vars_space)\n",
    "        n_lags = get_n_lags(**n_lags_space)\n",
    "        p_edge = get_p_edge(**p_edge_space(n_vars=n_vars, n_lags=n_lags))\n",
    "        scm = TempSCM(\n",
    "            method=\"C\",\n",
    "            n_vars=n_vars,\n",
    "            n_lags=n_lags,\n",
    "            p_edge=p_edge,\n",
    "            funcs=get_funcs(**funcs_space),\n",
    "            z_distributions=get_z_distribution(**z_distribution_space)   \n",
    "        )\n",
    "        ts_pre = scm.generate_time_series(n_samples=i_step)\n",
    "\n",
    "    # Simulate with TCS\n",
    "    true_data = ts_pre.copy()\n",
    "    results_tcs = get_optimal_sim_XY(true_data=true_data, sparsity_penalty=True)\n",
    "\n",
    "    # Make a copy of the true and predicted SCMs\n",
    "    true_scm = deepcopy(scm)\n",
    "    true_scm_i = deepcopy(scm)\n",
    "    tcs_scm = deepcopy(results_tcs[\"optimal_scm\"])\n",
    "    tcs_scm_i = deepcopy(results_tcs[\"optimal_scm\"])\n",
    "\n",
    "    true_scm._reset_time_series()\n",
    "    true_scm_i._reset_time_series()\n",
    "    tcs_scm._reset_time_series()\n",
    "    tcs_scm_i._reset_time_series()\n",
    "\n",
    "    # Define intervened nodes & perform the soft intervention\n",
    "    n2is = np.random.choice(a=range(len(scm.temp_nodes)), size=FNS, replace=False)\n",
    "    for n2i in n2is:\n",
    "        true_scm_i.temp_nodes[n2i].func = functools.partial(_torch_noisy_f, func=true_scm_i.temp_nodes[n2i].func)\n",
    "        tcs_scm_i.temp_nodes[n2i].func = functools.partial(_torch_noisy_f, func=tcs_scm_i.temp_nodes[n2i].func)\n",
    "\n",
    "    # Generate interventional data from the true SCM & the predicted SCM\n",
    "    ts_pre_true = true_scm.generate_time_series(n_samples=NS)\n",
    "    ts_pre_tcs = tcs_scm.generate_time_series(n_samples=NS)\n",
    "    ts_post_i_true = true_scm_i.generate_time_series(n_samples=NS)\n",
    "    ts_post_i_tcs = tcs_scm_i.generate_time_series(n_samples=NS)\n",
    "\n",
    "    # Evaluate\n",
    "    # auc_pre = run_detection_metrics(real=ts_pre_true, synthetic=ts_pre_tcs)['auc']\n",
    "    # auc_post = run_detection_metrics(real=ts_post_i_true, synthetic=ts_post_i_tcs)['auc']\n",
    "    auc_pre, _, _ = results_tcs[\"optimal_detector\"](real=ts_pre_true, synthetic=ts_pre_tcs, **results_tcs[\"optimal_detector_config\"])\n",
    "    auc_post, _, _ = results_tcs[\"optimal_detector\"](real=ts_post_i_true, synthetic=ts_post_i_tcs, **results_tcs[\"optimal_detector_config\"])\n",
    "    print(f\"AUC ori : {results_tcs['auc']}\")\n",
    "    print(f\"AUC pre : {auc_pre}\")\n",
    "    print(f\"AUC post : {auc_post}\")\n",
    "    auc_ori_list.append(round(results_tcs[\"auc\"], 2))\n",
    "    auc_pre_list.append(round(auc_pre, 2))\n",
    "    auc_post_list.append(round(auc_post, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf6147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "pre_post_auc = {'pre': auc_ori_list, 'post': auc_post_list}\n",
    "\n",
    "interventions_dir = Path(os.getcwd()).parents[1] / 'data' / 'results' / 'interventions'\n",
    "os.makedirs(interventions_dir, exist_ok=True)\n",
    "with open(interventions_dir / f\"pre_post_auc_{LOC}L_{SCALE}S.json\", \"w\") as f:\n",
    "    json.dump(pre_post_auc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f2b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check results\n",
    "with open(interventions_dir / f\"pre_post_auc_{LOC}L_{SCALE}S.json\", \"r\") as f:\n",
    "    pre_post_auc = json.load(f)\n",
    "\n",
    "print(np.mean(pre_post_auc[\"pre\"]).round(2))\n",
    "print(np.mean(auc_pre_list).round(2))\n",
    "print(np.mean(pre_post_auc[\"post\"]).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87c0861",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9be5c37",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c367c9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # - proof that on averace the return of TCS discrimination score is on average consisten with the run_detection_metric output\n",
    "# dl = []\n",
    "# for _ in trange(100):\n",
    "#     tcs_scm._reset_time_series()\n",
    "#     true_scm._reset_time_series()\n",
    "#     tcs_ts = tcs_scm.generate_time_series(n_samples=NS)\n",
    "#     true_ts = true_scm.generate_time_series(n_samples=NS)\n",
    "#     dl.append(run_detection_metrics(real=true_ts, synthetic=tcs_ts, verbose=False)[\"auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1390e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"TCS auc : {round(results_tcs['auc'], 2)}\")\n",
    "# print(f\"RDM auc : {np.mean(dl).round(2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
