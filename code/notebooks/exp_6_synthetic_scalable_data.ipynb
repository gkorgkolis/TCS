{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1fe7948",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "178e4984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimesFM v1.2.0. See https://github.com/google-research/timesfm/blob/master/README.md for updated APIs.\n",
      "Loaded Jax TimesFM.\n",
      "Loaded PyTorch TimesFM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting 1 CUDA device(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\skypl\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "[KeOps] Warning : Cuda libraries were not detected on the system or could not be loaded ; using cpu only mode\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import string\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from tempogen.temporal_scm import TempSCM\n",
    "from tempogen.functional_utils import (_torch_tanh, _torch_arctan, _torch_sin, _torch_cos,\n",
    "                                       _torch_sigmoid, _torch_pow, _torch_identity, _torch_sqrt)\n",
    "\n",
    "from tempogen.temporal_random_generation import get_p_edge, get_funcs, get_z_distribution \n",
    "from simulation.simulation_utils import simulate\n",
    "\n",
    "from cdt.metrics import SHD\n",
    "from simulation.simulation_metrics import mmd_torch\n",
    "\n",
    "from simulation.simulation_tools import get_optimal_sim_XY, run_detection_metrics_XY, prepare_det_data, run_detection_metrics\n",
    "from utils import custom_binary_metrics, _from_full_to_cp\n",
    "\n",
    "from CausalTime.tools import generate_CT\n",
    "from synthcity.plugins import Plugins\n",
    "from synthcity.plugins.core.dataloader import TimeSeriesDataLoader\n",
    "\n",
    "from sdv.metadata import Metadata\n",
    "from sdv.sequential import PARSynthesizer\n",
    "\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "def prind(di): print(json.dumps(di, sort_keys=False, indent=4))\n",
    "\n",
    "COL_NAMES = list(string.ascii_uppercase) + [\"\".join(a) for a in list(itertools.permutations(list(string.ascii_uppercase), r=2))]\n",
    "\n",
    "par_dir = Path(os.getcwd()).parents[1].as_posix()\n",
    "FN = \"cp_0\"\n",
    "SF = \"L_1L+-\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf018c2a",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ee5e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "par_dir = Path(os.getcwd()).parents[1].as_posix() \n",
    "FN = \"cp_0\"\n",
    "SF = \"L_1L+-\"\n",
    "N_SCMS = 10\n",
    "\n",
    "# # the space for the # vars during random generation\n",
    "n_vars_space = {\n",
    "    \"a\": np.arange(start=10, stop=20, step=1).tolist(), \n",
    "    \"p\": np.full(fill_value=1/10, shape=[10]).tolist()\n",
    "    # \"p\": [0.05, 0.05, 0.125, 0.125, 0.15, 0.15, 0.125, 0.125, 0.05, 0.05]\n",
    "}\n",
    "# - dynamic version of get_n_vars, able to favor specific size of graphs \n",
    "def get_n_vars(a, p):\n",
    "    return rng.choice(a=a, p=p)\n",
    "\n",
    "# # the space for the # lags during random generation\n",
    "n_lags_space = {\n",
    "    \"a\": [1, 2, 3], \n",
    "    \"p\": [1.0, 0.0, 0.0]\n",
    "}\n",
    "# - dynamic version of get_n_vars, able to favor specific size of graphs \n",
    "def get_n_lags(a, p):\n",
    "    return rng.choice(a=a, p=p)\n",
    "\n",
    "# the space for the edge probability during random generation \n",
    "# - dynamic version, depending on the # vars & # lags, to keep large graphs sparser; based on preconfigured options; still testing this\n",
    "def p_edge_space(n_vars, n_lags):\n",
    "    total_edges = (n_vars**2)*n_lags\n",
    "    if total_edges < 100:\n",
    "        return {\n",
    "            \"c\": None,\n",
    "            \"values\": [3/total_edges, 5/total_edges, 7/total_edges],\n",
    "            \"weights\": [0.6, 0.3, 0.1] \n",
    "        }\n",
    "    elif total_edges < 200:\n",
    "        return {\n",
    "            \"c\": None,\n",
    "            \"values\": [5/total_edges, 7/total_edges, 9/total_edges],\n",
    "            \"weights\": [0.6, 0.3, 0.1] \n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"c\": None,\n",
    "            \"values\": [9/total_edges, 12/total_edges, 15/total_edges],\n",
    "            \"weights\": [0.6, 0.3, 0.1] \n",
    "        }\n",
    "\n",
    "# the space for functional dependencies during random generation\n",
    "funcs_space = {\n",
    "    \"c\": None, \n",
    "    \"functions\": [_torch_identity, _torch_sqrt, _torch_sigmoid, _torch_sin, _torch_cos, _torch_arctan],\n",
    "    # \"weights\": [0.2, 0.2, 0.1, 0.1, 0.1, 0.3],\n",
    "    \"weights\": [0.6, 0.4, 0.0, 0.0, 0.0, 0.0],  \n",
    "}\n",
    "\n",
    "# the space for noise distribution during random generation\n",
    "z_distribution_space = {\n",
    "     \"c\": None,\n",
    "     \"functions\": [\n",
    "        torch.distributions.normal.Normal(loc=0, scale=0.005), \n",
    "        torch.distributions.uniform.Uniform(low=-0.15, high=0.15)],\n",
    "     \"weights\": [0.01, 0.99] \n",
    "}\n",
    "\n",
    "\n",
    "# main loop for random SCM generation\n",
    "for ctr in range(N_SCMS):\n",
    "\n",
    "    # DFs & SCMs\n",
    "    n_vars = get_n_vars(**n_vars_space)\n",
    "    n_lags = get_n_lags(**n_lags_space)\n",
    "    p_edge = get_p_edge(**p_edge_space(n_vars=n_vars, n_lags=n_lags))\n",
    "    scm = TempSCM(\n",
    "        method=\"C\",\n",
    "        n_vars=n_vars,\n",
    "        n_lags=n_lags,\n",
    "        p_edge=p_edge,\n",
    "        funcs=get_funcs(**funcs_space),\n",
    "        z_distributions=get_z_distribution(**z_distribution_space)   \n",
    "    )\n",
    "    df = scm.generate_time_series(n_samples=500) \n",
    "\n",
    "    # resample DFs & SCMs in case of infinity values\n",
    "    while np.isinf(df.values.sum()):\n",
    "        n_vars = get_n_vars(**n_vars_space)\n",
    "        n_lags = get_n_lags(**n_lags_space)\n",
    "        p_edge = get_p_edge(**p_edge_space(n_vars=n_vars, n_lags=n_lags))\n",
    "        scm = TempSCM(\n",
    "            method=\"C\",\n",
    "            n_vars=n_vars,\n",
    "            n_lags=n_lags,\n",
    "            p_edge=p_edge,\n",
    "            funcs=get_funcs(**funcs_space),\n",
    "            z_distributions=get_z_distribution(**z_distribution_space)   \n",
    "        )\n",
    "        df = scm.generate_time_series(n_samples=500)\n",
    "\n",
    "    # # store\n",
    "    # Path(f\"{par_dir}/data/cp_style/{FN}_{SF}/data/\").mkdir(parents=True, exist_ok=True)\n",
    "    # Path(f\"{par_dir}/data/cp_style/{FN}_{SF}/structure/\").mkdir(parents=True, exist_ok=True)\n",
    "    # df.to_csv(f\"{par_dir}/data/cp_style/{FN}_{SF}/data/cp_collection_data_{ctr}.csv\", index=False)\n",
    "    # torch.save(scm.causal_structure.causal_structure_cp, f\"{par_dir}/data/cp_style/{FN}_{SF}/structure/cp_collection_struct_{ctr}.pt\")\n",
    "\n",
    "    # plot\n",
    "    scm.causal_structure.plot_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb27791",
   "metadata": {},
   "source": [
    "### Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798ad278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data structure as such for convenient comparison with CausalTime\n",
    "DATA_DICT = {\n",
    "    filename.split(\".csv\")[0]: {\n",
    "        'data_path': f\"{par_dir}/data/cp_style/{FN}_{SF}/data/\",\n",
    "        'data_type': 'fmri',\n",
    "        'task': filename, \n",
    "        'straight_path': f\"{par_dir}/data/cp_style/{FN}_{SF}/data/\" + f\"{filename}\",\n",
    "        'struct_path' : f\"{par_dir}/data/cp_style/{FN}_{SF}/structure/\" + f\"{filename.replace('data', 'struct').replace('csv', 'pt')}\"\n",
    "    } for filename in os.listdir(f\"{par_dir}/data/cp_style/{FN}_{SF}/data/\")\n",
    "}\n",
    "\n",
    "# CausalTime Parameters\n",
    "PARAMS = {\n",
    "    \"batch_size\" : 32, \n",
    "    \"hidden_size\" : 128, \n",
    "    \"num_layers\" : 2, \n",
    "    \"dropout\" : 0.1, \n",
    "    \"seq_length\" : 20, \n",
    "    \"test_size\" : 0.2, \n",
    "    \"learning_rate\" : 0.0001, \n",
    "    \"n_epochs\" : 1, \n",
    "    \"flow_length\" : 4, \n",
    "    \"gen_n\" : 20, \n",
    "    \"n\" : 2000,\n",
    "    \"arch_type\" : \"MLP\", \n",
    "    \"save_path\" : \"outputs/\", \n",
    "    \"log_dir\" : \"log/\", \n",
    "}\n",
    "\n",
    "# placeholders\n",
    "shd_dict = {}\n",
    "\n",
    "auc_dict_tcs = {}\n",
    "data_dict_tcs = {}\n",
    "auc_dict_ct = {}\n",
    "data_dict_ct = {}\n",
    "auc_dict_sdv = {}\n",
    "data_dict_sdv = {}\n",
    "auc_dict_tvae = {}\n",
    "data_dict_tvae = {}\n",
    "\n",
    "mmd_dict_tcs = {}\n",
    "mmd_dict_ct = {}\n",
    "mmd_dict_sdv = {}\n",
    "mmd_dict_tvae = {}\n",
    "\n",
    "# run\n",
    "for k, v in list(DATA_DICT.items())[:]:\n",
    "\n",
    "    try:\n",
    "    \n",
    "        # info\n",
    "        filename = v['task']\n",
    "        print(f\" \\n------------- {filename} ---------------\\n \")\n",
    "\n",
    "        # data\n",
    "        true_data = pd.read_csv(v[\"straight_path\"])\n",
    "        true_data = true_data.rename(columns=dict(zip(true_data.columns, COL_NAMES[:true_data.shape[1]])))\n",
    "        true_graph = torch.load(v[\"struct_path\"])\n",
    "        \n",
    "        # adjust timesteps for computation time (1000 max)\n",
    "        print(f\"true data length: {true_data.shape[0]}\")\n",
    "\n",
    "        if true_data.shape[0]>2000:\n",
    "            anchor = np.random.uniform(low=0, high=true_data.shape[0]-2000)\n",
    "            true_data = true_data.loc[anchor : anchor + 2000, :]\n",
    "            print(f\"true data length (adjusted): {true_data.shape[0]}\")\n",
    "\n",
    "        # # adjust zeros for the numerical computations of some methods\n",
    "        # for i in range(true_data.shape[0]):\n",
    "        #     for j in range(true_data.shape[1]):\n",
    "        #         if true_data.iloc[i, j] == 0:\n",
    "        #             true_data.iloc[i, j] += np.random.uniform(low=0.0001, high=0.001)\n",
    "        \n",
    "\n",
    "        print(\"\"\"\\n ____________________________________ Simulate w/ CausalTime ____________________________________ \\n\"\"\")\n",
    "\n",
    "        true_pd, pro_true_pd, skimmed_pd, pro_gen_pd = generate_CT(\n",
    "                batch_size=PARAMS[\"batch_size\"], \n",
    "                hidden_size=PARAMS[\"hidden_size\"], \n",
    "                num_layers=PARAMS[\"num_layers\"], \n",
    "                dropout=PARAMS[\"dropout\"], \n",
    "                seq_length=PARAMS[\"seq_length\"], \n",
    "                test_size=PARAMS[\"test_size\"], \n",
    "                learning_rate=PARAMS[\"learning_rate\"], \n",
    "                n_epochs=PARAMS[\"n_epochs\"], \n",
    "                flow_length=PARAMS[\"flow_length\"], \n",
    "                gen_n=PARAMS[\"gen_n\"], \n",
    "                n=PARAMS[\"n\"],\n",
    "                arch_type=PARAMS[\"arch_type\"], \n",
    "                save_path=PARAMS[\"save_path\"], \n",
    "                log_dir=PARAMS[\"log_dir\"], \n",
    "                data_path=v[\"data_path\"],\n",
    "                data_type= v[\"data_type\"], \n",
    "                task= v[\"task\"],\n",
    "            )\n",
    "        ct_data = pro_gen_pd.copy()\n",
    "\n",
    "        # Fix potential length mismatches\n",
    "        if true_data.shape[0] > ct_data.shape[0]:\n",
    "            true_data = true_data[:ct_data.shape[0]]\n",
    "        elif true_data.shape[0] < ct_data.shape[0]:\n",
    "            ct_data = ct_data[:true_data.shape[0]]\n",
    "\n",
    "        # Evaluate\n",
    "        print(f\"LOG : true shape - {true_data.shape} VS ct shape - {ct_data.shape}\")\n",
    "        # train_X, train_Y, test_X, test_Y = prepare_det_data(real=true_data, synthetic=ct_data)\n",
    "        # ct_auc = run_detection_metrics_XY(train_X=train_X, train_Y=train_Y, test_X=test_X, test_Y=test_Y)['auc']\n",
    "        ct_auc = run_detection_metrics(real=true_data, synthetic=ct_data, bias_correction=False, verbose=False)['auc']\n",
    "\n",
    "        mmd = mmd_torch(synthetic=ct_data, real=true_data)\n",
    "\n",
    "        # Store\n",
    "        mmd_dict_ct[filename] = mmd\n",
    "        auc_dict_ct[filename] = ct_auc\n",
    "        data_dict_ct[filename] = ct_data.copy()\n",
    "\n",
    "\n",
    "        print(\"\"\"\\n ____________ Simulate w/ SDV ____________ \\n\"\"\")\n",
    "\n",
    "        true_data_sdv = true_data.copy()\n",
    "\n",
    "        # Creating same conditions as CausalTime\n",
    "        els = true_data_sdv.shape[0] % (true_data_sdv.shape[0]//20)\n",
    "        if els!=0:\n",
    "            true_data_sdv = true_data_sdv.iloc[:-els, :]\n",
    "\n",
    "        # Sequence key\n",
    "        true_data_sdv.loc[:, 'id'] = [i for i in range(true_data_sdv.shape[0]//20) for _ in range(20)]\n",
    "\n",
    "        # Metadata\n",
    "        metadata = Metadata.detect_from_dataframe(data=true_data_sdv)\n",
    "        metadata.tables[\"table\"].columns[\"id\"][\"sdtype\"] = \"id\"\n",
    "        metadata.set_sequence_key(column_name='id')\n",
    "\n",
    "        # Synthesizer\n",
    "        synthesizer = PARSynthesizer(metadata)\n",
    "        synthesizer.fit(data=true_data_sdv)\n",
    "        synthetic_data = synthesizer.sample(num_sequences=true_data_sdv.shape[0]//20 + 1)\n",
    "\n",
    "        # Fix potential length mismatches\n",
    "        sdv_data = synthetic_data.loc[:len(true_data), :].drop(columns=[\"id\"])\n",
    "        if true_data.shape[0] > sdv_data.shape[0]:\n",
    "            true_data = true_data[:sdv_data.shape[0]]\n",
    "        elif true_data.shape[0] < sdv_data.shape[0]:\n",
    "            sdv_data = sdv_data[:true_data.shape[0]]\n",
    "        \n",
    "        mmd = mmd_torch(synthetic=sdv_data, real=true_data)\n",
    "\n",
    "        # Evaluate\n",
    "        print(f\"LOG : true shape - {true_data.shape} VS sdv shape - {sdv_data.shape}\")\n",
    "        # train_X, train_Y, test_X, test_Y = prepare_det_data(real=true_data, synthetic=sdv_data)\n",
    "        # sdv_auc = run_detection_metrics_XY(train_X=train_X, train_Y=train_Y, test_X=test_X, test_Y=test_Y)['auc']\n",
    "        sdv_auc = run_detection_metrics(real=true_data, synthetic=sdv_data, bias_correction=False, verbose=False)['auc']\n",
    "\n",
    "        # Store\n",
    "        mmd_dict_sdv[filename] = mmd\n",
    "        auc_dict_sdv[filename] = sdv_auc\n",
    "        data_dict_sdv[filename] = sdv_data.copy()\n",
    "\n",
    "\n",
    "        print(\"\"\"\\n _____________ Simulate w/ TCS _____________ \\n\"\"\")\n",
    "\n",
    "        results_tcs = get_optimal_sim_XY(true_data=true_data)\n",
    "        tcs_data = results_tcs[\"optimal_data\"]\n",
    "        tcs_auc = results_tcs[\"auc\"]\n",
    "        pred_graph = results_tcs[\"optimal_scm\"].causal_structure.causal_structure_cp\n",
    "\n",
    "        # Fix potential length mismatches\n",
    "        if true_data.shape[0] > tcs_data.shape[0]:\n",
    "            true_data = true_data[:tcs_data.shape[0]]\n",
    "        elif true_data.shape[0] < tcs_data.shape[0]:\n",
    "            tcs_data = tcs_data[:true_data.shape[0]]\n",
    "\n",
    "        # Evaluate\n",
    "        print(f\"LOG : true shape - {true_data.shape} VS tcs shape - {tcs_data.shape}\")\n",
    "        # train_X, train_Y, test_X, test_Y = prepare_det_data(real=true_data, synthetic=tcs_data)\n",
    "        # tcs_auc = run_detection_metrics_XY(train_X=train_X, train_Y=train_Y, test_X=test_X, test_Y=test_Y)\n",
    "        tcs_auc = run_detection_metrics(real=true_data, synthetic=tcs_data, bias_correction=False, verbose=False)['auc']\n",
    "\n",
    "        mmd = mmd_torch(synthetic=tcs_data, real=true_data)\n",
    "\n",
    "        # Store\n",
    "        mmd_dict_tcs[filename] = mmd\n",
    "        auc_dict_tcs[filename] = tcs_auc\n",
    "        data_dict_tcs[filename] = tcs_data.copy()\n",
    "        shd_dict[filename] = SHD(true_graph.numpy(), pred_graph.numpy())\n",
    "        \n",
    "        \n",
    "        print(\"\"\"\\n _____________ Simulate w/ TimeVAE _____________ \\n\"\"\")\n",
    "        \n",
    "        # Prepare TimeVAE Data\n",
    "        dat = true_data.copy()\n",
    "\n",
    "        n_samples = dat.shape[0]\n",
    "        if 'target' in dat.columns:\n",
    "            X = dat.drop(columns=['target']) \n",
    "            y = dat['target'] \n",
    "        else:\n",
    "            X = dat\n",
    "            y = None\n",
    "\n",
    "        temporal_data = [X]\n",
    "        observation_times = [X.index.to_numpy()]\n",
    "\n",
    "        # Initialize the TimeSeriesDataLoader\n",
    "        X_loader = TimeSeriesDataLoader(\n",
    "            temporal_data=temporal_data, \n",
    "            observation_times=observation_times, \n",
    "            outcome=y,\n",
    "            static_data=None,\n",
    "            train_size=1.0, \n",
    "            test_size=0.0\n",
    "        )\n",
    "\n",
    "        # Define plugin kwargs for TimeVAE\n",
    "        plugin_kwargs = dict(\n",
    "            n_iter=30,\n",
    "            batch_size=64,\n",
    "            lr=0.001,\n",
    "            encoder_n_layers_hidden=2,\n",
    "            decoder_n_layers_hidden=2,\n",
    "            encoder_dropout=0.05,\n",
    "            decoder_dropout=0.05\n",
    "        )\n",
    "\n",
    "        # Initialize the generative model for TimeVAE\n",
    "        test_plugin = Plugins().get(\"tvae\", **plugin_kwargs)\n",
    "        # test_plugin = Plugins().get(\"timegan\", ?)\n",
    "\n",
    "        # Fit the model\n",
    "        if y is not None:\n",
    "            test_plugin.fit(X_loader, cond=y)\n",
    "        else:\n",
    "            test_plugin.fit(X_loader)\n",
    "\n",
    "        # Generate synthetic data\n",
    "        generated_data = test_plugin.generate(count=n_samples) \n",
    "\n",
    "        # Extract the generated time-series data\n",
    "        generated_data = generated_data.data[\"seq_data\"]\n",
    "\n",
    "        # Drop unnecessary columns like \"seq_id\", \"seq_time_id\"\n",
    "        generated_data = generated_data.drop(columns=[\"seq_id\", \"seq_time_id\"])\n",
    "\n",
    "        # Fix potential length mismatches\n",
    "        if true_data.shape[0] > generated_data.shape[0]:\n",
    "            true_data = true_data[:generated_data.shape[0]]\n",
    "        elif true_data.shape[0] < generated_data.shape[0]:\n",
    "            generated_data = generated_data[:true_data.shape[0]]\n",
    "\n",
    "        # Evaluate TimeVAE generated data\n",
    "        print(f\"LOG : true shape - {true_data.shape} VS generated shape - {generated_data.shape}\")\n",
    "        # train_X, train_Y, test_X, test_Y = prepare_det_data(real=true_data, synthetic=generated_data)\n",
    "        # tvae_auc = run_detection_metrics_XY(train_X=train_X, train_Y=train_Y, test_X=test_X, test_Y=test_Y)['auc']\n",
    "        tvae_auc = run_detection_metrics(real=true_data, synthetic=generated_data, bias_correction=False, verbose=False)['auc']\n",
    "\n",
    "        mmd = mmd_torch(synthetic=generated_data, real=dat)\n",
    "\n",
    "        # Store results for TimeVAE\n",
    "        mmd_dict_tvae[filename] = mmd\n",
    "        auc_dict_tvae[filename] = tvae_auc\n",
    "        data_dict_tvae[filename] = generated_data.copy()\n",
    "    \n",
    "    except:\n",
    "        print(\" -------------------------- OUPS -------------------------- \")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eb7607",
   "metadata": {},
   "source": [
    "### Store and visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f70601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat\n",
    "save_path = Path(os.getcwd()).parents[1] / \"data\" / \"results\" / \"vs\"\n",
    "\n",
    "auc_dict_tcs\n",
    "auc_dict = {}\n",
    "mmd_dict = {}\n",
    "\n",
    "for k in list(auc_dict_tcs.keys())[:]:\n",
    "    auc_dict[k] = {\n",
    "        \"tcs\" : auc_dict_tcs[k] if k in auc_dict_tcs.keys() else 0, \n",
    "        \"ct\" : auc_dict_ct[k] if k in auc_dict_ct.keys() else 0,\n",
    "        \"cpar\" : auc_dict_sdv[k] if k in auc_dict_sdv.keys() else 0,\n",
    "        \"tvae\" : auc_dict_tvae[k] if k in auc_dict_tvae.keys() else 0,\n",
    "    }\n",
    "\n",
    "for k in list(mmd_dict_tcs.keys())[:]:\n",
    "    mmd_dict[k] = {\n",
    "        \"tcs\" : mmd_dict_tcs[k] if k in mmd_dict_tcs.keys() else 0, \n",
    "        \"ct\" : mmd_dict_ct[k] if k in mmd_dict_ct.keys() else 0,\n",
    "        \"cpar\" : mmd_dict_sdv[k] if k in mmd_dict_sdv.keys() else 0,\n",
    "        \"tvae\" : mmd_dict_tvae[k] if k in mmd_dict_tvae.keys() else 0,\n",
    "    }\n",
    "\n",
    "# Store as JSON \n",
    "json.dump(auc_dict, open(save_path / f\"{FN}_{SF}_auc.json\", \"w\"))\n",
    "json.dump(mmd_dict, open(save_path / f\"{FN}_{SF}_mmd.json\", \"w\"))\n",
    "json.dump(shd_dict, open(save_path / f\"{FN}_{SF}_shd.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2962b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON\n",
    "save_path = Path(os.getcwd()).parents[1] / \"data\" / \"results\" / \"vs\"\n",
    "\n",
    "auc_dict = json.load(open(save_path / f\"{FN}_{SF}_auc.json\", \"r\"))\n",
    "mmd_dict = json.load(open(save_path / f\"{FN}_{SF}_mmd.json\", \"r\"))\n",
    "shd_dict = json.load(open(save_path / f\"{FN}_{SF}_shd.json\", \"r\"))\n",
    "auc_df = pd.DataFrame(auc_dict).T\n",
    "mmd_df = pd.DataFrame(mmd_dict).T\n",
    "auc_df = auc_df.loc[~(auc_df==0).any(axis=1)]\n",
    "mmd_df = mmd_df.loc[~(mmd_df==0).any(axis=1)]\n",
    "\n",
    "# Plot\n",
    "f, axs = plt.subplots(ncols=2, figsize=(12, 6))\n",
    "sns.boxplot(data=auc_df, palette=\"pastel\", ax=axs[0])\n",
    "axs[0].set_title(r\"AUC$_D$ comparison\")\n",
    "sns.boxplot(data=mmd_df, palette=\"pastel\", ax=axs[1])\n",
    "axs[1].set_title(\"MMD comparison\")\n",
    "# import matplotlib\n",
    "# patches = [matplotlib.patches.Patch(color=sns.color_palette(\"pastel\")[i], label=t) \n",
    "#            for i,t in enumerate(t.get_text() for t in axs[1].get_xticklabels())]\n",
    "# plt.legend(handles=patches, loc=\"upper right\")\n",
    "plt.show()\n",
    "\n",
    "prind(shd_dict)\n",
    "auc_df[\"tcs\"]\n",
    "auc_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
