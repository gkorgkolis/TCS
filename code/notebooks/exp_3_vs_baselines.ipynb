{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from sdv.metadata import Metadata\n",
    "from sdv.sequential import PARSynthesizer\n",
    "\n",
    "from simulation.simulation_tools import get_optimal_sim_XY, run_detection_metrics, run_detection_metrics_XY, prepare_det_data\n",
    "from simulation.simulation_metrics import mmd_torch\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from CausalTime.tools import generate_CT\n",
    "\n",
    "from synthcity.plugins import Plugins\n",
    "from synthcity.plugins.core.dataloader import TimeSeriesDataLoader\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_dir = Path(os.getcwd()).parents[1].as_posix() \n",
    "\n",
    "# Data structure as such for convenient comparison with CausalTime\n",
    "DATA_DICT = {\n",
    "    filename.split(\".csv\")[0]: {\n",
    "        'data_path': f\"{par_dir}/data/fMRI/timeseries/\",\n",
    "        'data_type': 'fmri',\n",
    "        'task': filename, \n",
    "        'straight_path': f\"{par_dir}/data/fMRI/timeseries/\" + f\"{filename}\"\n",
    "    } for filename in os.listdir(f\"{par_dir}/data/fMRI/timeseries/\")\n",
    "\n",
    "    # filename.split(\".csv\")[0]: {\n",
    "    #     'data_path': f\"{par_dir}/data/cp_style/increasing_edges_cp_1/data/\",\n",
    "    #     'data_type': 'fmri',\n",
    "    #     'task': filename, \n",
    "    #     'straight_path': f\"{par_dir}/data/cp_style/increasing_edges_cp_1/data/\" + f\"{filename}\"\n",
    "    # } for filename in os.listdir(f\"{par_dir}/data/cp_style/increasing_edges_cp_1/data/\")\n",
    "}\n",
    "\n",
    "# CausalTime Parameters\n",
    "PARAMS = {\n",
    "    \"batch_size\" : 32, \n",
    "    \"hidden_size\" : 128, \n",
    "    \"num_layers\" : 2, \n",
    "    \"dropout\" : 0.1, \n",
    "    \"seq_length\" : 20, \n",
    "    \"test_size\" : 0.2, \n",
    "    \"learning_rate\" : 0.0001, \n",
    "    \"n_epochs\" : 1, \n",
    "    \"flow_length\" : 4, \n",
    "    \"gen_n\" : 20, \n",
    "    \"n\" : 2000,\n",
    "    \"arch_type\" : \"MLP\", \n",
    "    \"save_path\" : \"outputs/\", \n",
    "    \"log_dir\" : \"log/\", \n",
    "}\n",
    "\n",
    "# Placeholders\n",
    "det_dict = {}\n",
    "auc_dict_tcs = {}\n",
    "data_dict_tcs = {}\n",
    "auc_dict_ct = {}\n",
    "data_dict_ct = {}\n",
    "auc_dict_sdv = {}\n",
    "data_dict_sdv = {}\n",
    "auc_dict_tvae = {}\n",
    "data_dict_tvae = {}\n",
    "\n",
    "mmd_dict_tcs = {}\n",
    "mmd_dict_ct = {}\n",
    "mmd_dict_sdv = {}\n",
    "mmd_dict_tvae = {}\n",
    "\n",
    "for k, v in list(DATA_DICT.items())[:]:\n",
    "    \n",
    "    # info\n",
    "    filename = v['task']\n",
    "    print(f\" \\n------------- {filename} ---------------\\n \")\n",
    "\n",
    "    # data\n",
    "    true_data = pd.read_csv(v[\"straight_path\"])\n",
    "    \n",
    "    # adjust timesteps for computation time (1000 max)\n",
    "    print(f\"true data length: {true_data.shape[0]}\")\n",
    "    if true_data.shape[0]>1000:\n",
    "        anchor = np.random.uniform(low=0, high=true_data.shape[0]-1000)\n",
    "        true_data = true_data.loc[anchor : anchor + 1000, :]\n",
    "        print(f\"true data length (adjusted): {true_data.shape[0]}\")\n",
    "    \n",
    "\n",
    "    \"\"\" ____________________________________ Simulate w/ TCS ____________________________________ \"\"\"\n",
    "\n",
    "    results_tcs = get_optimal_sim_XY(true_data=true_data)\n",
    "    tcs_data = results_tcs[\"optimal_data\"]\n",
    "    tcs_auc = results_tcs[\"auc\"]\n",
    "\n",
    "    \"\"\" Get optimal det & config \"\"\"\n",
    "    optimal_det_config = results_tcs[\"optimal_detector_config\"]\n",
    "    optimal_det_func = results_tcs[\"optimal_detector\"]\n",
    "\n",
    "    # Fix potential length mismatches\n",
    "    if true_data.shape[0] > tcs_data.shape[0]:\n",
    "        true_data = true_data[:tcs_data.shape[0]]\n",
    "    elif true_data.shape[0] < tcs_data.shape[0]:\n",
    "        tcs_data = tcs_data[:true_data.shape[0]]\n",
    "\n",
    "    # Evaluate\n",
    "    train_X, train_Y, test_X, test_Y = prepare_det_data(real=true_data, synthetic=tcs_data)\n",
    "    tcs_auc = run_detection_metrics_XY(train_X=train_X, train_Y=train_Y, test_X=test_X, test_Y=test_Y)['auc']\n",
    "\n",
    "    mmd = mmd_torch(synthetic=tcs_data, real=true_data)\n",
    "\n",
    "    # Store\n",
    "    mmd_dict_tcs[filename] = mmd\n",
    "    auc_dict_tcs[filename] = tcs_auc\n",
    "    data_dict_tcs[filename] = tcs_data.copy()\n",
    "\n",
    "\n",
    "    print(\"\"\"\\n ____________________________________ Simulate w/ CausalTime ____________________________________ \\n\"\"\")\n",
    "\n",
    "    true_pd, pro_true_pd, skimmed_pd, pro_gen_pd = generate_CT(\n",
    "            batch_size=PARAMS[\"batch_size\"], \n",
    "            hidden_size=PARAMS[\"hidden_size\"], \n",
    "            num_layers=PARAMS[\"num_layers\"], \n",
    "            dropout=PARAMS[\"dropout\"], \n",
    "            seq_length=PARAMS[\"seq_length\"], \n",
    "            test_size=PARAMS[\"test_size\"], \n",
    "            learning_rate=PARAMS[\"learning_rate\"], \n",
    "            n_epochs=PARAMS[\"n_epochs\"], \n",
    "            flow_length=PARAMS[\"flow_length\"], \n",
    "            gen_n=PARAMS[\"gen_n\"], \n",
    "            n=PARAMS[\"n\"],\n",
    "            arch_type=PARAMS[\"arch_type\"], \n",
    "            save_path=PARAMS[\"save_path\"], \n",
    "            log_dir=PARAMS[\"log_dir\"], \n",
    "            data_path=v[\"data_path\"],\n",
    "            data_type= v[\"data_type\"], \n",
    "            task= v[\"task\"],\n",
    "        )\n",
    "    ct_data = pro_gen_pd.copy()\n",
    "\n",
    "    # Fix potential length mismatches\n",
    "    if true_data.shape[0] > ct_data.shape[0]:\n",
    "        true_data = true_data[:ct_data.shape[0]]\n",
    "    elif true_data.shape[0] < ct_data.shape[0]:\n",
    "        ct_data = ct_data[:true_data.shape[0]]\n",
    "\n",
    "    # Evaluate\n",
    "    train_X, train_Y, test_X, test_Y = prepare_det_data(real=true_data, synthetic=ct_data)\n",
    "    ct_auc = run_detection_metrics_XY(train_X=train_X, train_Y=train_Y, test_X=test_X, test_Y=test_Y)['auc']\n",
    "\n",
    "    mmd = mmd_torch(synthetic=ct_data, real=true_data)\n",
    "\n",
    "    # Store\n",
    "    mmd_dict_ct[filename] = mmd\n",
    "    auc_dict_ct[filename] = ct_auc\n",
    "    data_dict_ct[filename] = ct_data.copy()\n",
    "\n",
    "\n",
    "    print(\"\"\"\\n ____________ Simulate w/ SDV ____________ \\n\"\"\")\n",
    "\n",
    "    true_data_sdv = true_data.copy()\n",
    "\n",
    "    # Creating same conditions as CausalTime\n",
    "    els = true_data_sdv.shape[0] % (true_data_sdv.shape[0]//20)\n",
    "    if els!=0:\n",
    "        true_data_sdv = true_data_sdv.loc[:-els, :]\n",
    "\n",
    "    # Sequence key\n",
    "    true_data_sdv.loc[:, 'id'] = [i for i in range(true_data_sdv.shape[0]//20) for _ in range(20)]\n",
    "\n",
    "    # Metadata\n",
    "    metadata = Metadata.detect_from_dataframe(data=true_data_sdv)\n",
    "    metadata.tables[\"table\"].columns[\"id\"][\"sdtype\"] = \"id\"\n",
    "    metadata.set_sequence_key(column_name='id')\n",
    "\n",
    "    # Synthesizer\n",
    "    synthesizer = PARSynthesizer(metadata)\n",
    "    synthesizer.fit(data=true_data_sdv)\n",
    "    synthetic_data = synthesizer.sample(num_sequences=true_data_sdv.shape[0]//20 + 1)\n",
    "\n",
    "    # Fix potential length mismatches\n",
    "    sdv_data = synthetic_data.loc[:len(true_data), :].drop(columns=[\"id\"])\n",
    "    if true_data.shape[0] > sdv_data.shape[0]:\n",
    "        true_data = true_data[:sdv_data.shape[0]]\n",
    "    elif true_data.shape[0] < sdv_data.shape[0]:\n",
    "        sdv_data = sdv_data[:true_data.shape[0]]\n",
    "    \n",
    "    mmd = mmd_torch(synthetic=sdv_data, real=true_data)\n",
    "\n",
    "    # Evaluate\n",
    "    train_X, train_Y, test_X, test_Y = prepare_det_data(real=true_data, synthetic=sdv_data)\n",
    "    sdv_auc = run_detection_metrics_XY(train_X=train_X, train_Y=train_Y, test_X=test_X, test_Y=test_Y)['auc']\n",
    "\n",
    "    # Store\n",
    "    mmd_dict_sdv[filename] = mmd\n",
    "    auc_dict_sdv[filename] = sdv_auc\n",
    "    data_dict_sdv[filename] = sdv_data.copy()\n",
    "\n",
    "\n",
    "    print(\"\"\"\\n _____________ Simulate w/ TimeVAE _____________ \\n\"\"\")\n",
    "    \n",
    "    # Prepare TimeVAE Data\n",
    "    dat = true_data.copy()\n",
    "\n",
    "    n_samples = dat.shape[0]\n",
    "    if 'target' in dat.columns:\n",
    "        X = dat.drop(columns=['target']) \n",
    "        y = dat['target'] \n",
    "    else:\n",
    "        X = dat\n",
    "        y = None\n",
    "\n",
    "    temporal_data = [X]\n",
    "    observation_times = [X.index.to_numpy()]\n",
    "\n",
    "    # Initialize the TimeSeriesDataLoader\n",
    "    X_loader = TimeSeriesDataLoader(\n",
    "        temporal_data=temporal_data, \n",
    "        observation_times=observation_times, \n",
    "        outcome=y,\n",
    "        static_data=None,\n",
    "        train_size=1.0, \n",
    "        test_size=0.0\n",
    "    )\n",
    "\n",
    "    # Define plugin kwargs for TimeVAE\n",
    "    plugin_kwargs = dict(\n",
    "        n_iter=30,\n",
    "        batch_size=64,\n",
    "        lr=0.001,\n",
    "        encoder_n_layers_hidden=2,\n",
    "        decoder_n_layers_hidden=2,\n",
    "        encoder_dropout=0.05,\n",
    "        decoder_dropout=0.05\n",
    "    )\n",
    "\n",
    "    # Initialize the generative model for TimeVAE\n",
    "    test_plugin = Plugins().get(\"tvae\", **plugin_kwargs)\n",
    "    # test_plugin = Plugins().get(\"timegan\", ?)\n",
    "\n",
    "    # Fit the model\n",
    "    if y is not None:\n",
    "        test_plugin.fit(X_loader, cond=y)\n",
    "    else:\n",
    "        test_plugin.fit(X_loader)\n",
    "\n",
    "    # Generate synthetic data\n",
    "    generated_data = test_plugin.generate(count=n_samples) \n",
    "\n",
    "    # Extract the generated time-series data\n",
    "    generated_data = generated_data.data[\"seq_data\"]\n",
    "\n",
    "    # Drop unnecessary columns like \"seq_id\", \"seq_time_id\"\n",
    "    generated_data = generated_data.drop(columns=[\"seq_id\", \"seq_time_id\"])\n",
    "\n",
    "    # Fix potential length mismatches\n",
    "    if true_data.shape[0] > generated_data.shape[0]:\n",
    "        true_data = true_data[:generated_data.shape[0]]\n",
    "    elif true_data.shape[0] < generated_data.shape[0]:\n",
    "        generated_data = generated_data[:true_data.shape[0]]\n",
    "\n",
    "    # Evaluate TimeVAE generated data\n",
    "    train_X, train_Y, test_X, test_Y = prepare_det_data(real=true_data, synthetic=generated_data)\n",
    "    tvae_auc = run_detection_metrics_XY(train_X=train_X, train_Y=train_Y, test_X=test_X, test_Y=test_Y)['auc']\n",
    "\n",
    "    mmd = mmd_torch(synthetic=generated_data, real=dat)\n",
    "\n",
    "    # Store results for TimeVAE\n",
    "    mmd_dict_tvae[filename] = mmd\n",
    "    auc_dict_tvae[filename] = tvae_auc\n",
    "    data_dict_tvae[filename] = generated_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"TCS Mean AUC: {np.array(list(auc_dict_tcs.values())).mean()}\")\n",
    "print(f\"TCS Var AUC: {np.array(list(auc_dict_tcs.values())).var()}\")\n",
    "\n",
    "print(f\"TCS Mean MMD: {np.array(list(mmd_dict_tcs.values())).mean()}\")\n",
    "print(f\"TCS Var MMD: {np.array(list(mmd_dict_tcs.values())).var()}\")\n",
    "\n",
    "print(f\"CT Mean AUC: {np.array(list(auc_dict_ct.values())).mean()}\")\n",
    "print(f\"CT Var AUC: {np.array(list(auc_dict_ct.values())).var()}\")\n",
    "\n",
    "print(f\"CT Mean MMD: {np.array(list(mmd_dict_ct.values())).mean()}\")\n",
    "print(f\"CT Var MMD: {np.array(list(mmd_dict_ct.values())).var()}\")\n",
    "\n",
    "print(f\"CPAR Mean AUC: {np.array(list(auc_dict_sdv.values())).mean()}\")\n",
    "print(f\"CPAR Var AUC: {np.array(list(auc_dict_sdv.values())).var()}\")\n",
    "\n",
    "print(f\"CPAR Mean MMD: {np.array(list(mmd_dict_sdv.values())).mean()}\")\n",
    "print(f\"CPAR Var MMD: {np.array(list(mmd_dict_sdv.values())).var()}\")\n",
    "\n",
    "print(f\"TimeVAE Mean AUC: {np.array(list(auc_dict_tvae.values())).mean()}\")\n",
    "print(f\"TimeVAE Var AUC: {np.array(list(auc_dict_tvae.values())).var()}\")\n",
    "\n",
    "print(f\"TimeVAE Mean MMD: {np.array(list(mmd_dict_tvae.values())).mean()}\")\n",
    "print(f\"TimeVAE Var MMD: {np.array(list(mmd_dict_tvae.values())).var()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat\n",
    "save_path = Path(os.getcwd()).parents[1] / \"data\" / \"results\" / \"vs\"\n",
    "\n",
    "auc_dict_tcs\n",
    "auc_dict = {}\n",
    "mmd_dict = {}\n",
    "\n",
    "for k in list(auc_dict_tcs.keys())[:2]:\n",
    "    auc_dict[k] = {\n",
    "        \"tcs\" : auc_dict_tcs[k], \n",
    "        \"ct\" : auc_dict_ct[k],\n",
    "        \"cpar\" : auc_dict_sdv[k],\n",
    "        \"tvae\" : auc_dict_tvae[k],\n",
    "    }\n",
    "\n",
    "for k in list(mmd_dict_tcs.keys())[:2]:\n",
    "    mmd_dict[k] = {\n",
    "        \"tcs\" : mmd_dict_tcs[k], \n",
    "        \"ct\" : mmd_dict_ct[k],\n",
    "        \"cpar\" : mmd_dict_sdv[k],\n",
    "        \"tvae\" : mmd_dict_tvae[k],\n",
    "    }\n",
    "\n",
    "auc_dict\n",
    "\n",
    "# Save in JSON \n",
    "json.dump(auc_dict, open(save_path / \"fmri_auc.json\", \"w\"))\n",
    "json.dump(mmd_dict, open(save_path / \"fmri_mmd.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
