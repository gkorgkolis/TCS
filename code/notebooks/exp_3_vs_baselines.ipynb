{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import string\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from sdv.metadata import Metadata\n",
    "from sdv.sequential import PARSynthesizer\n",
    "\n",
    "from simulation.simulation_tools import get_optimal_sim_XY, run_detection_metrics, run_detection_metrics_XY, prepare_det_data\n",
    "from simulation.simulation_metrics import mmd_torch\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from CausalTime.tools import generate_CT\n",
    "\n",
    "from synthcity.plugins import Plugins\n",
    "from synthcity.plugins.core.dataloader import TimeSeriesDataLoader\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "COL_NAMES = list(string.ascii_uppercase) + [\"\".join(a) for a in list(itertools.permutations(list(string.ascii_uppercase), r=2))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_dir = Path(os.getcwd()).parents[1].as_posix() \n",
    "FN = \"AirQualityUCI\"\n",
    "\n",
    "# Data structure as such for convenient comparison with CausalTime\n",
    "DATA_DICT = {\n",
    "    # # NOTE: uncomment for MvTS real data\n",
    "    filename.split(\".csv\")[0]: {\n",
    "        'data_path': f\"{par_dir}/data/MvTS/{FN}/\",\n",
    "        'data_type': 'fmri',\n",
    "        'task': filename, \n",
    "        'straight_path': f\"{par_dir}/data/MvTS/{FN}/\" + f\"{filename}\"\n",
    "    } for filename in os.listdir(f\"{par_dir}/data/MvTS/{FN}/\")\n",
    "\n",
    "    # # NOTE: uncomment for the rest data\n",
    "    # filename.split(\".csv\")[0]: {\n",
    "    #     'data_path': f\"{par_dir}/data/{FN}/\",\n",
    "    #     'data_type': 'fmri',\n",
    "    #     'task': filename, \n",
    "    #     'straight_path': f\"{par_dir}/data/{FN}/\" + f\"{filename}\"\n",
    "    # } for filename in os.listdir(f\"{par_dir}/data/{FN}/\")\n",
    "}\n",
    "\n",
    "# CausalTime Parameters\n",
    "PARAMS = {\n",
    "    \"batch_size\" : 32, \n",
    "    \"hidden_size\" : 128, \n",
    "    \"num_layers\" : 2, \n",
    "    \"dropout\" : 0.1, \n",
    "    \"seq_length\" : 20, \n",
    "    \"test_size\" : 0.2, \n",
    "    \"learning_rate\" : 0.0001, \n",
    "    \"n_epochs\" : 1, \n",
    "    \"flow_length\" : 4, \n",
    "    \"gen_n\" : 20, \n",
    "    \"n\" : 2000,\n",
    "    \"arch_type\" : \"MLP\", \n",
    "    \"save_path\" : \"outputs/\", \n",
    "    \"log_dir\" : \"log/\", \n",
    "}\n",
    "\n",
    "# Placeholders\n",
    "det_dict = {}\n",
    "auc_dict_tcs = {}\n",
    "data_dict_tcs = {}\n",
    "auc_dict_ct = {}\n",
    "data_dict_ct = {}\n",
    "auc_dict_sdv = {}\n",
    "data_dict_sdv = {}\n",
    "auc_dict_tvae = {}\n",
    "data_dict_tvae = {}\n",
    "\n",
    "mmd_dict_tcs = {}\n",
    "mmd_dict_ct = {}\n",
    "mmd_dict_sdv = {}\n",
    "mmd_dict_tvae = {}\n",
    "\n",
    "for k, v in list(DATA_DICT.items())[:]:\n",
    "\n",
    "    try:\n",
    "    \n",
    "        # info\n",
    "        filename = v['task']\n",
    "        print(f\" \\n------------- {filename} ---------------\\n \")\n",
    "\n",
    "        # data\n",
    "        true_data = pd.read_csv(v[\"straight_path\"])\n",
    "        true_data = true_data.rename(columns=dict(zip(true_data.columns, COL_NAMES[:true_data.shape[1]])))\n",
    "        \n",
    "        # adjust timesteps for computation time (1000 max)\n",
    "        print(f\"true data length: {true_data.shape[0]}\")\n",
    "\n",
    "        if true_data.shape[0]>2000:\n",
    "            anchor = np.random.uniform(low=0, high=true_data.shape[0]-2000)\n",
    "            true_data = true_data.loc[anchor : anchor + 2000, :]\n",
    "            print(f\"true data length (adjusted): {true_data.shape[0]}\")\n",
    "\n",
    "        for i in range(true_data.shape[0]):\n",
    "            for j in range(true_data.shape[1]):\n",
    "                if true_data.iloc[i, j] == 0:\n",
    "                    true_data.iloc[i, j] += np.random.uniform(low=0.0001, high=0.001)\n",
    "        \n",
    "\n",
    "        \"\"\" ____________________________________ Simulate w/ TCS ____________________________________ \"\"\"\n",
    "\n",
    "        results_tcs = get_optimal_sim_XY(true_data=true_data)\n",
    "        tcs_data = results_tcs[\"optimal_data\"]\n",
    "        tcs_auc = results_tcs[\"auc\"]\n",
    "\n",
    "        \"\"\" Get optimal det & config \"\"\"\n",
    "        optimal_det_config = results_tcs[\"optimal_detector_config\"]\n",
    "        optimal_det_func = results_tcs[\"optimal_detector\"]\n",
    "\n",
    "        # Fix potential length mismatches\n",
    "        if true_data.shape[0] > tcs_data.shape[0]:\n",
    "            true_data = true_data[:tcs_data.shape[0]]\n",
    "        elif true_data.shape[0] < tcs_data.shape[0]:\n",
    "            tcs_data = tcs_data[:true_data.shape[0]]\n",
    "\n",
    "        # Evaluate\n",
    "        print(f\"LOG : true shape - {true_data.shape} VS tcs shape - {tcs_data.shape}\")\n",
    "        train_X, train_Y, test_X, test_Y = prepare_det_data(real=true_data, synthetic=tcs_data)\n",
    "        tcs_auc = run_detection_metrics_XY(train_X=train_X, train_Y=train_Y, test_X=test_X, test_Y=test_Y)['auc']\n",
    "\n",
    "        mmd = mmd_torch(synthetic=tcs_data, real=true_data)\n",
    "\n",
    "        # Store\n",
    "        mmd_dict_tcs[filename] = mmd\n",
    "        auc_dict_tcs[filename] = tcs_auc\n",
    "        data_dict_tcs[filename] = tcs_data.copy()\n",
    "\n",
    "\n",
    "        print(\"\"\"\\n ____________________________________ Simulate w/ CausalTime ____________________________________ \\n\"\"\")\n",
    "\n",
    "        true_pd, pro_true_pd, skimmed_pd, pro_gen_pd = generate_CT(\n",
    "                batch_size=PARAMS[\"batch_size\"], \n",
    "                hidden_size=PARAMS[\"hidden_size\"], \n",
    "                num_layers=PARAMS[\"num_layers\"], \n",
    "                dropout=PARAMS[\"dropout\"], \n",
    "                seq_length=PARAMS[\"seq_length\"], \n",
    "                test_size=PARAMS[\"test_size\"], \n",
    "                learning_rate=PARAMS[\"learning_rate\"], \n",
    "                n_epochs=PARAMS[\"n_epochs\"], \n",
    "                flow_length=PARAMS[\"flow_length\"], \n",
    "                gen_n=PARAMS[\"gen_n\"], \n",
    "                n=PARAMS[\"n\"],\n",
    "                arch_type=PARAMS[\"arch_type\"], \n",
    "                save_path=PARAMS[\"save_path\"], \n",
    "                log_dir=PARAMS[\"log_dir\"], \n",
    "                data_path=v[\"data_path\"],\n",
    "                data_type= v[\"data_type\"], \n",
    "                task= v[\"task\"],\n",
    "            )\n",
    "        ct_data = pro_gen_pd.copy()\n",
    "\n",
    "        # Fix potential length mismatches\n",
    "        if true_data.shape[0] > ct_data.shape[0]:\n",
    "            true_data = true_data[:ct_data.shape[0]]\n",
    "        elif true_data.shape[0] < ct_data.shape[0]:\n",
    "            ct_data = ct_data[:true_data.shape[0]]\n",
    "\n",
    "        # Evaluate\n",
    "        print(f\"LOG : true shape - {true_data.shape} VS ct shape - {ct_data.shape}\")\n",
    "        train_X, train_Y, test_X, test_Y = prepare_det_data(real=true_data, synthetic=ct_data)\n",
    "        ct_auc = run_detection_metrics_XY(train_X=train_X, train_Y=train_Y, test_X=test_X, test_Y=test_Y)['auc']\n",
    "\n",
    "        mmd = mmd_torch(synthetic=ct_data, real=true_data)\n",
    "\n",
    "        # Store\n",
    "        mmd_dict_ct[filename] = mmd\n",
    "        auc_dict_ct[filename] = ct_auc\n",
    "        data_dict_ct[filename] = ct_data.copy()\n",
    "\n",
    "\n",
    "        print(\"\"\"\\n ____________ Simulate w/ SDV ____________ \\n\"\"\")\n",
    "\n",
    "        true_data_sdv = true_data.copy()\n",
    "\n",
    "        # Creating same conditions as CausalTime\n",
    "        els = true_data_sdv.shape[0] % (true_data_sdv.shape[0]//20)\n",
    "        if els!=0:\n",
    "            true_data_sdv = true_data_sdv.iloc[:-els, :]\n",
    "\n",
    "        # Sequence key\n",
    "        true_data_sdv.loc[:, 'id'] = [i for i in range(true_data_sdv.shape[0]//20) for _ in range(20)]\n",
    "\n",
    "        # Metadata\n",
    "        metadata = Metadata.detect_from_dataframe(data=true_data_sdv)\n",
    "        metadata.tables[\"table\"].columns[\"id\"][\"sdtype\"] = \"id\"\n",
    "        metadata.set_sequence_key(column_name='id')\n",
    "\n",
    "        # Synthesizer\n",
    "        synthesizer = PARSynthesizer(metadata)\n",
    "        synthesizer.fit(data=true_data_sdv)\n",
    "        synthetic_data = synthesizer.sample(num_sequences=true_data_sdv.shape[0]//20 + 1)\n",
    "\n",
    "        # Fix potential length mismatches\n",
    "        sdv_data = synthetic_data.loc[:len(true_data), :].drop(columns=[\"id\"])\n",
    "        if true_data.shape[0] > sdv_data.shape[0]:\n",
    "            true_data = true_data[:sdv_data.shape[0]]\n",
    "        elif true_data.shape[0] < sdv_data.shape[0]:\n",
    "            sdv_data = sdv_data[:true_data.shape[0]]\n",
    "        \n",
    "        mmd = mmd_torch(synthetic=sdv_data, real=true_data)\n",
    "\n",
    "        # Evaluate\n",
    "        print(f\"LOG : true shape - {true_data.shape} VS sdv shape - {sdv_data.shape}\")\n",
    "        train_X, train_Y, test_X, test_Y = prepare_det_data(real=true_data, synthetic=sdv_data)\n",
    "        sdv_auc = run_detection_metrics_XY(train_X=train_X, train_Y=train_Y, test_X=test_X, test_Y=test_Y)['auc']\n",
    "\n",
    "        # Store\n",
    "        mmd_dict_sdv[filename] = mmd\n",
    "        auc_dict_sdv[filename] = sdv_auc\n",
    "        data_dict_sdv[filename] = sdv_data.copy()\n",
    "\n",
    "\n",
    "        print(\"\"\"\\n _____________ Simulate w/ TimeVAE _____________ \\n\"\"\")\n",
    "        \n",
    "        # Prepare TimeVAE Data\n",
    "        dat = true_data.copy()\n",
    "\n",
    "        n_samples = dat.shape[0]\n",
    "        if 'target' in dat.columns:\n",
    "            X = dat.drop(columns=['target']) \n",
    "            y = dat['target'] \n",
    "        else:\n",
    "            X = dat\n",
    "            y = None\n",
    "\n",
    "        temporal_data = [X]\n",
    "        observation_times = [X.index.to_numpy()]\n",
    "\n",
    "        # Initialize the TimeSeriesDataLoader\n",
    "        X_loader = TimeSeriesDataLoader(\n",
    "            temporal_data=temporal_data, \n",
    "            observation_times=observation_times, \n",
    "            outcome=y,\n",
    "            static_data=None,\n",
    "            train_size=1.0, \n",
    "            test_size=0.0\n",
    "        )\n",
    "\n",
    "        # Define plugin kwargs for TimeVAE\n",
    "        plugin_kwargs = dict(\n",
    "            n_iter=30,\n",
    "            batch_size=64,\n",
    "            lr=0.001,\n",
    "            encoder_n_layers_hidden=2,\n",
    "            decoder_n_layers_hidden=2,\n",
    "            encoder_dropout=0.05,\n",
    "            decoder_dropout=0.05\n",
    "        )\n",
    "\n",
    "        # Initialize the generative model for TimeVAE\n",
    "        test_plugin = Plugins().get(\"tvae\", **plugin_kwargs)\n",
    "        # test_plugin = Plugins().get(\"timegan\", ?)\n",
    "\n",
    "        # Fit the model\n",
    "        if y is not None:\n",
    "            test_plugin.fit(X_loader, cond=y)\n",
    "        else:\n",
    "            test_plugin.fit(X_loader)\n",
    "\n",
    "        # Generate synthetic data\n",
    "        generated_data = test_plugin.generate(count=n_samples) \n",
    "\n",
    "        # Extract the generated time-series data\n",
    "        generated_data = generated_data.data[\"seq_data\"]\n",
    "\n",
    "        # Drop unnecessary columns like \"seq_id\", \"seq_time_id\"\n",
    "        generated_data = generated_data.drop(columns=[\"seq_id\", \"seq_time_id\"])\n",
    "\n",
    "        # Fix potential length mismatches\n",
    "        if true_data.shape[0] > generated_data.shape[0]:\n",
    "            true_data = true_data[:generated_data.shape[0]]\n",
    "        elif true_data.shape[0] < generated_data.shape[0]:\n",
    "            generated_data = generated_data[:true_data.shape[0]]\n",
    "\n",
    "        # Evaluate TimeVAE generated data\n",
    "        print(f\"LOG : true shape - {true_data.shape} VS generated shape - {generated_data.shape}\")\n",
    "        train_X, train_Y, test_X, test_Y = prepare_det_data(real=true_data, synthetic=generated_data)\n",
    "        tvae_auc = run_detection_metrics_XY(train_X=train_X, train_Y=train_Y, test_X=test_X, test_Y=test_Y)['auc']\n",
    "\n",
    "        mmd = mmd_torch(synthetic=generated_data, real=dat)\n",
    "\n",
    "        # Store results for TimeVAE\n",
    "        mmd_dict_tvae[filename] = mmd\n",
    "        auc_dict_tvae[filename] = tvae_auc\n",
    "        data_dict_tvae[filename] = generated_data.copy()\n",
    "    \n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"TCS Mean AUC: {np.array(list(auc_dict_tcs.values())).mean()}\")\n",
    "print(f\"TCS Var AUC: {np.array(list(auc_dict_tcs.values())).var()}\")\n",
    "\n",
    "print(f\"TCS Mean MMD: {np.array(list(mmd_dict_tcs.values())).mean()}\")\n",
    "print(f\"TCS Var MMD: {np.array(list(mmd_dict_tcs.values())).var()}\")\n",
    "\n",
    "print(f\"CT Mean AUC: {np.array(list(auc_dict_ct.values())).mean()}\")\n",
    "print(f\"CT Var AUC: {np.array(list(auc_dict_ct.values())).var()}\")\n",
    "\n",
    "print(f\"CT Mean MMD: {np.array(list(mmd_dict_ct.values())).mean()}\")\n",
    "print(f\"CT Var MMD: {np.array(list(mmd_dict_ct.values())).var()}\")\n",
    "\n",
    "print(f\"CPAR Mean AUC: {np.array(list(auc_dict_sdv.values())).mean()}\")\n",
    "print(f\"CPAR Var AUC: {np.array(list(auc_dict_sdv.values())).var()}\")\n",
    "\n",
    "print(f\"CPAR Mean MMD: {np.array(list(mmd_dict_sdv.values())).mean()}\")\n",
    "print(f\"CPAR Var MMD: {np.array(list(mmd_dict_sdv.values())).var()}\")\n",
    "\n",
    "print(f\"TimeVAE Mean AUC: {np.array(list(auc_dict_tvae.values())).mean()}\")\n",
    "print(f\"TimeVAE Var AUC: {np.array(list(auc_dict_tvae.values())).var()}\")\n",
    "\n",
    "print(f\"TimeVAE Mean MMD: {np.array(list(mmd_dict_tvae.values())).mean()}\")\n",
    "print(f\"TimeVAE Var MMD: {np.array(list(mmd_dict_tvae.values())).var()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store and visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat\n",
    "save_path = Path(os.getcwd()).parents[1] / \"data\" / \"results\" / \"vs\"\n",
    "\n",
    "auc_dict_tcs\n",
    "auc_dict = {}\n",
    "mmd_dict = {}\n",
    "\n",
    "for k in list(auc_dict_tcs.keys())[:]:\n",
    "    auc_dict[k] = {\n",
    "        \"tcs\" : auc_dict_tcs[k], \n",
    "        \"ct\" : auc_dict_ct[k],\n",
    "        \"cpar\" : auc_dict_sdv[k],\n",
    "        \"tvae\" : auc_dict_tvae[k],\n",
    "    }\n",
    "\n",
    "for k in list(mmd_dict_tcs.keys())[:]:\n",
    "    mmd_dict[k] = {\n",
    "        \"tcs\" : mmd_dict_tcs[k], \n",
    "        \"ct\" : mmd_dict_ct[k],\n",
    "        \"cpar\" : mmd_dict_sdv[k],\n",
    "        \"tvae\" : mmd_dict_tvae[k],\n",
    "    }\n",
    "\n",
    "\n",
    "# Store as JSON \n",
    "json.dump(auc_dict, open(save_path / f\"{FN}_auc.json\", \"w\"))\n",
    "json.dump(mmd_dict, open(save_path / f\"{FN}_mmd.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON\n",
    "save_path = Path(os.getcwd()).parents[1] / \"data\" / \"results\" / \"vs\"\n",
    "\n",
    "auc_dict = json.load(open(save_path / f\"{FN}_auc.json\", \"r\"))\n",
    "mmd_dict = json.load(open(save_path / f\"{FN}_mmd.json\", \"r\"))\n",
    "auc_df = pd.DataFrame(auc_dict).T\n",
    "mmd_df = pd.DataFrame(mmd_dict).T\n",
    "\n",
    "# Plot\n",
    "f, axs = plt.subplots(ncols=2, figsize=(12, 6))\n",
    "sns.barplot(data=auc_df, palette=\"pastel\", ax=axs[0])\n",
    "axs[0].set_title(\"AUC comparison\")\n",
    "sns.barplot(data=mmd_df, palette=\"pastel\", ax=axs[1])\n",
    "axs[1].set_title(\"MMD comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helping code to create block-bootstraped versions of a large dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "SET_LEN = 2000\n",
    "FN = \"AirQualityUCI\"\n",
    "\n",
    "# Path\n",
    "par_dir = Path(os.getcwd()).parents[1].as_posix() \n",
    "target_path = f\"{par_dir}/data/MvTS/{FN}\"\n",
    "\n",
    "# Data\n",
    "original = pd.read_csv(target_path + f'/{FN}_original.csv')\n",
    "original = original.rename(columns=dict(zip(\n",
    "    original.columns, \n",
    "    COL_NAMES\n",
    ")))\n",
    "\n",
    "# adjust timesteps for computation time (1500 max)\n",
    "print(f\"true data length: {original.shape[0]}\")\n",
    "if original.shape[0]>SET_LEN:\n",
    "    for b in range(20):\n",
    "        anchor = np.random.uniform(low=0, high=original.shape[0] - SET_LEN)\n",
    "        boot = original.loc[anchor : anchor + SET_LEN, :]\n",
    "        print(f\"true data length (adjusted): {boot.shape[0]}\")\n",
    "        boot.to_csv(target_path + f\"/{FN}_boot_{b}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
