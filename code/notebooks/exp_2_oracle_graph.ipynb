{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "import string\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from cdt.metrics import SHD\n",
    "\n",
    "from simulation.simulation_tools import get_optimal_sim_XY, get_optimal_sim_XY_dual\n",
    "from utils import custom_binary_metrics, _from_full_to_cp, _from_cp_to_full, regular_order_pd\n",
    "\n",
    "from simulation.simulation_configs import cd_config as CD_CONFIGS\n",
    "from simulation.simulation_configs import pred_config as PRED_CONFIGS\n",
    "from simulation.simulation_configs import noise_config as NOISE_CONFIGS\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "COL_NAMES = list(string.ascii_uppercase) + [\"\".join(a) for a in list(itertools.permutations(list(string.ascii_uppercase), r=2))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment \n",
    "Observe the behavior of TCS when given the ground truth graph (oracle) as the 1st phase's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path\n",
    "ie_custom_path = list(Path(\".\").resolve().parents)[1] / \"data\" / \"cp_style\" / \"increasing_edges_cp_1\"\n",
    "\n",
    "\n",
    "# configs & oracle\n",
    "CONFIGS = [\n",
    "    {\n",
    "        \"cd\": cd_method, \n",
    "        \"fc\": fc_method,\n",
    "        \"z\": z_approximation,\n",
    "        \"o\": z_approximation\n",
    "    } for cd_method in list(CD_CONFIGS.values()) \n",
    "    for fc_method in list(PRED_CONFIGS.values())\n",
    "    for z_approximation in list(NOISE_CONFIGS.values())\n",
    "]\n",
    "\n",
    "\n",
    "# placeholders\n",
    "errors = []\n",
    "scm_list = {}\n",
    "shd_list = {}\n",
    "scores_list = {}\n",
    "det_auc_list = {}\n",
    "struct_auc_list = {}\n",
    "\n",
    "errors_w = []\n",
    "scm_list_w = {}\n",
    "shd_list_w = {}\n",
    "scores_list_w = {}\n",
    "det_auc_list_w = {}\n",
    "struct_auc_list_w = {}\n",
    "\n",
    "\n",
    "\n",
    "# run loop\n",
    "for fn in os.listdir(ie_custom_path / \"data\"):\n",
    "        print(f\"\\n\\n------------------------------------- {fn} -------------------------------------\")\n",
    "\n",
    "        # read the time-series\n",
    "        X_data = pd.read_csv(ie_custom_path / \"data\" / fn)\n",
    "        X_data.rename(columns=dict(zip(X_data.columns, COL_NAMES[:X_data.shape[1]])), inplace=True)\n",
    "\n",
    "        # read the ground truth\n",
    "        gn = fn.split(\"_ts\")[0] + \"_struct.pt\"\n",
    "        Y_data = torch.load(ie_custom_path / \"structure\" / gn)\n",
    "        Y_data[Y_data > 0] = 1      # carefull here\n",
    "        Y_data_pd = _from_cp_to_full(Y_data)\n",
    "        Y_data_pd = Y_data_pd.loc[regular_order_pd(Y_data_pd), regular_order_pd(Y_data_pd)]\n",
    "        # print(f\"- edges : {Y_data.sum().int()}\")\n",
    "\n",
    "        CD_CONFIGS['ORACLE_1'] = {\n",
    "                \"cd_method\": \"ORACLE\",\n",
    "                \"cd_kwargs\": {\n",
    "                        \"oracle\": Y_data_pd.copy()\n",
    "                }\n",
    "        }\n",
    "\n",
    "        # optimal simulation\n",
    "        res, res_s = get_optimal_sim_XY_dual(\n",
    "                true_data = X_data, \n",
    "                CONFIGS = None, \n",
    "                done_eval = False,\n",
    "                optimal_det_config = None,\n",
    "                optimal_det_func = None, \n",
    "                sparsity_penalty=True,\n",
    "                verbose = False\n",
    "        )\n",
    "\n",
    "        # compare\n",
    "        if isinstance(res[\"optimal_scm\"], pd.DataFrame):\n",
    "                pred_cp = _from_full_to_cp(res[\"optimal_scm\"])\n",
    "        else:\n",
    "                pred_cp = res[\"optimal_scm\"].causal_structure.causal_structure_cp\n",
    "        true_cp = Y_data\n",
    "        if  true_cp.shape[2]>pred_cp.shape[2]:\n",
    "                pred_cp = torch.nn.functional.pad(input=pred_cp, pad=(0, true_cp.shape[2] - pred_cp.shape[2], 0, 0, 0, 0), value=0)\n",
    "        if  pred_cp.shape[2]>true_cp.shape[2]:\n",
    "                true_cp = torch.nn.functional.pad(input=true_cp, pad=(0, pred_cp.shape[2] - true_cp.shape[2], 0, 0, 0, 0), value=0)\n",
    "        tpr, fpr, tnr, fnr, auc = custom_binary_metrics(binary=pred_cp, A=true_cp, verbose=True)\n",
    "        shd_d = SHD(target=true_cp.numpy(), pred=pred_cp.numpy(), double_for_anticausal=True)\n",
    "\n",
    "        # store\n",
    "        scm_list[fn] =  pred_cp\n",
    "        scores_list[fn] =  res[\"scores\"]\n",
    "        det_auc_list[fn] =  res[\"auc\"]\n",
    "        struct_auc_list[fn] =  {\"tpr\": tpr, \"fpr\": fpr, \"tnr\": tnr, \"fnr\": fnr, \"auc\": auc, \"shd\": shd_d,\n",
    "                                \"pred#\": pred_cp.sum().numpy(), \"true#\": true_cp.sum().numpy()}\n",
    "        shd_list[fn] = shd_d\n",
    "\n",
    "        # ___________________________________________________________________________________________\n",
    "\n",
    "\n",
    "        # compare\n",
    "        if isinstance(res_s[\"optimal_scm\"], pd.DataFrame):\n",
    "                pred_cp = _from_full_to_cp(res_s[\"optimal_scm\"])\n",
    "        else:\n",
    "                pred_cp = res_s[\"optimal_scm\"].causal_structure.causal_structure_cp\n",
    "        true_cp = Y_data\n",
    "        if  true_cp.shape[2]>pred_cp.shape[2]:\n",
    "                pred_cp = torch.nn.functional.pad(input=pred_cp, pad=(0, true_cp.shape[2] - pred_cp.shape[2], 0, 0, 0, 0), value=0)\n",
    "        if  pred_cp.shape[2]>true_cp.shape[2]:\n",
    "                true_cp = torch.nn.functional.pad(input=true_cp, pad=(0, pred_cp.shape[2] - true_cp.shape[2], 0, 0, 0, 0), value=0)\n",
    "        tpr, fpr, tnr, fnr, auc = custom_binary_metrics(binary=pred_cp, A=true_cp, verbose=True)\n",
    "        shd_d = SHD(target=true_cp.numpy(), pred=pred_cp.numpy(), double_for_anticausal=True)\n",
    "\n",
    "        # store\n",
    "        scm_list_w[fn] =  pred_cp\n",
    "        scores_list_w[fn] =  res_s[\"scores\"]\n",
    "        det_auc_list_w[fn] =  res_s[\"auc\"]\n",
    "        struct_auc_list_w[fn] =  {\"tpr\": tpr, \"fpr\": fpr, \"tnr\": tnr, \"fnr\": fnr, \"auc\": auc, \"shd\": shd_d,\n",
    "                                \"pred#\": pred_cp.sum().numpy(), \"true#\": true_cp.sum().numpy()}\n",
    "        shd_list_w[fn] = shd_d\n",
    "\n",
    "        # ___________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge\n",
    "res_both = {\n",
    "    \"res\" : struct_auc_list,\n",
    "    \"res_w\" : struct_auc_list_w, \n",
    "}\n",
    "\n",
    "# store results\n",
    "pickle.dump(res_both, open(\"data\" / \"results\" / \"oracle_graph\" / \"res_cp_ora_1.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "res_both = pickle.load(open(list(Path(\".\").resolve().parents)[1] / \"data\" / \"results\" / \"oracle_graph\" / \"res_cp_ora_1.p\", \"rb\"))\n",
    "\n",
    "# sort results according to ground truth edge density\n",
    "sorted_true, sorted_shd = list(zip(*sorted(list(zip(\n",
    "    [v[\"true#\"] for v in list(res_both['res'].values())], \n",
    "    [v[\"shd\"] for v in list(res_both['res'].values())])), key=lambda x: x[0])[:])) \n",
    "\n",
    "sorted_true_w, sorted_shd_w = list(zip(*sorted(list(zip(\n",
    "    [v[\"true#\"] for v in list(res_both['res_w'].values())], \n",
    "    [v[\"shd\"] for v in list(res_both['res_w'].values())])), key=lambda x: x[0])[:])) \n",
    "\n",
    "# plot\n",
    "f, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.scatter(x=sorted_true, y=sorted_shd, label=\"w/o sparsity penalty\", color=\"darkolivegreen\")\n",
    "ax.plot(sorted_true, sorted_shd, \"--\", color=\"darkolivegreen\", alpha=0.3)\n",
    "ax.scatter(x=sorted_true_w, y=sorted_shd_w, label=\"w/ sparsity penalty\", color='indianred')\n",
    "ax.plot(sorted_true_w, sorted_shd_w, \"--\", color='indianred', alpha=0.3)\n",
    "ax.set_xticks(list(set([int(x) for x in sorted_true])))\n",
    "ax.set_xticklabels(list(set([int(x) for x in sorted_true])))\n",
    "ax.set_xlabel(\"#edges\", fontdict={\"size\": 15})\n",
    "ax.set_ylabel(\"shd\", fontdict={\"size\": 15})\n",
    "# ax.set_title(\"SHD on synthetic data w/ increasing # of edges.\")\n",
    "ax.legend(fontsize=15)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
