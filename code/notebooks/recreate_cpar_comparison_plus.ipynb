{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimesFM v1.2.0. See https://github.com/google-research/timesfm/blob/master/README.md for updated APIs.\n",
      "Loaded Jax TimesFM.\n",
      "Loaded PyTorch TimesFM.\n",
      "WARNING:tensorflow:From C:\\Users\\skypl\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "[KeOps] Warning : Cuda libraries were not detected on the system or could not be loaded ; using cpu only mode\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from sdv.metadata import Metadata\n",
    "from sdv.sequential import PARSynthesizer\n",
    "\n",
    "from simulation.simulation_tools import get_optimal_sim_XY, run_detection_metrics\n",
    "from simulation.simulation_metrics import mmd_torch\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from CausalTime.tools import generate_CT\n",
    "\n",
    "from synthcity.plugins import Plugins\n",
    "from synthcity.plugins.core.dataloader import TimeSeriesDataLoader\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "------------- timeseries19.csv ---------------\n",
      " \n",
      "LOG: Optimal simulation: 6 are to be tested ...\n",
      "_____________________________________________________________________________________________________________\n",
      "_______________________ {'cd': {'cd_method': 'CP', 'cd_kwargs': {'model': '../cd_methods/CausalPretraining/res/deep_CI_RH_12_3_merged_290k.ckpt', 'model_name': 'deep_CI_RH_12_3_merged_290k', 'MAX_VAR': 12, 'thresholded': True, 'threshold': 0.05, 'enforce_density': False, 'density': [2, 10]}}, 'fc': {'pred_method': 'TCDF', 'pred_kwargs': {}}, 'z': {'noise_approximation': 'est'}, 'o': {'noise_approximation': 'est'}} _______________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 634.05it/s]\n",
      "100%|██████████| 1000/1000 [00:01<00:00, 689.10it/s]\n",
      "100%|██████████| 1000/1000 [00:01<00:00, 711.33it/s]\n",
      "100%|██████████| 1000/1000 [00:01<00:00, 694.58it/s]\n",
      "100%|██████████| 1000/1000 [00:01<00:00, 663.36it/s]\n",
      " 44%|████▍     | 1062/2420 [00:06<00:07, 171.77it/s]"
     ]
    }
   ],
   "source": [
    "par_dir = Path(os.getcwd()).parents[1].as_posix() \n",
    "# print(f\"data_path -> {data_path}\")\n",
    "\n",
    "# Data structure as such for convenient comparison with CausalTime\n",
    "DATA_DICT = {\n",
    "    filename.split(\".csv\")[0]: {\n",
    "        'data_path': f\"{par_dir}/data/fMRI/timeseries/\",\n",
    "        'data_type': 'fmri',\n",
    "        'task': filename, \n",
    "        'straight_path': f\"{par_dir}/data/fMRI/timeseries/\" + f\"{filename}\"\n",
    "    } for filename in os.listdir(f\"{par_dir}/data/fMRI/timeseries/\")\n",
    "}\n",
    "\n",
    "# CausalTime Parameters\n",
    "PARAMS = {\n",
    "    \"batch_size\" : 32, \n",
    "    \"hidden_size\" : 128, \n",
    "    \"num_layers\" : 2, \n",
    "    \"dropout\" : 0.1, \n",
    "    \"seq_length\" : 20, \n",
    "    \"test_size\" : 0.2, \n",
    "    \"learning_rate\" : 0.0001, \n",
    "    \"n_epochs\" : 1, \n",
    "    \"flow_length\" : 4, \n",
    "    \"gen_n\" : 20, \n",
    "    \"n\" : 2000,\n",
    "    \"arch_type\" : \"MLP\", \n",
    "    \"save_path\" : \"outputs/\", \n",
    "    \"log_dir\" : \"log/\", \n",
    "}\n",
    "\n",
    "# Placeholders\n",
    "det_dict = {}\n",
    "auc_dict_tcs = {}\n",
    "data_dict_tcs = {}\n",
    "auc_dict_ct = {}\n",
    "data_dict_ct = {}\n",
    "auc_dict_sdv = {}\n",
    "data_dict_sdv = {}\n",
    "auc_dict_tvae = {}\n",
    "data_dict_tvae = {}\n",
    "\n",
    "mmd_dict_tcs = {}\n",
    "mmd_dict_ct = {}\n",
    "mmd_dict_sdv = {}\n",
    "mmd_dict_tvae = {}\n",
    "\n",
    "for k, v in list(DATA_DICT.items())[:2]:\n",
    "    \n",
    "    # info\n",
    "    filename = v['task']\n",
    "    print(f\" \\n------------- {filename} ---------------\\n \")\n",
    "\n",
    "    # data\n",
    "    true_data = pd.read_csv(v[\"straight_path\"])\n",
    "\n",
    "\n",
    "    \"\"\" ____________________________________ Simulate w/ TCS ____________________________________ \"\"\"\n",
    "\n",
    "    results_tcs = get_optimal_sim_XY(true_data=true_data)\n",
    "    tcs_data = results_tcs[\"optimal_data\"]\n",
    "    tcs_auc = results_tcs[\"auc\"]\n",
    "\n",
    "    \"\"\" Get optimal det & config \"\"\"\n",
    "    optimal_det_config = results_tcs[\"optimal_detector_config\"]\n",
    "    optimal_det_func = results_tcs[\"optimal_detector\"]\n",
    "\n",
    "    # Fix potential length mismatches\n",
    "    if true_data.shape[0] > tcs_data.shape[0]:\n",
    "        true_data = true_data[:tcs_data.shape[0]]\n",
    "    elif true_data.shape[0] < tcs_data.shape[0]:\n",
    "        tcs_data = tcs_data[:true_data.shape[0]]\n",
    "\n",
    "    # Evaluate\n",
    "    tcs_auc = run_detection_metrics(\n",
    "        real=true_data, \n",
    "        synthetic=tcs_data, \n",
    "    )['auc']\n",
    "\n",
    "    mmd = mmd_torch(synthetic=tcs_data, real=true_data)\n",
    "\n",
    "    # Store\n",
    "    mmd_dict_tcs[filename] = mmd\n",
    "    auc_dict_tcs[filename] = tcs_auc\n",
    "    data_dict_tcs[filename] = tcs_data.copy()\n",
    "\n",
    "\n",
    "    print(\"\"\"\\n ____________________________________ Simulate w/ CausalTime ____________________________________ \\n\"\"\")\n",
    "\n",
    "    true_pd, pro_true_pd, skimmed_pd, pro_gen_pd = generate_CT(\n",
    "            batch_size=PARAMS[\"batch_size\"], \n",
    "            hidden_size=PARAMS[\"hidden_size\"], \n",
    "            num_layers=PARAMS[\"num_layers\"], \n",
    "            dropout=PARAMS[\"dropout\"], \n",
    "            seq_length=PARAMS[\"seq_length\"], \n",
    "            test_size=PARAMS[\"test_size\"], \n",
    "            learning_rate=PARAMS[\"learning_rate\"], \n",
    "            n_epochs=PARAMS[\"n_epochs\"], \n",
    "            flow_length=PARAMS[\"flow_length\"], \n",
    "            gen_n=PARAMS[\"gen_n\"], \n",
    "            n=PARAMS[\"n\"],\n",
    "            arch_type=PARAMS[\"arch_type\"], \n",
    "            save_path=PARAMS[\"save_path\"], \n",
    "            log_dir=PARAMS[\"log_dir\"], \n",
    "            data_path=v[\"data_path\"],\n",
    "            data_type= v[\"data_type\"], \n",
    "            task= v[\"task\"],\n",
    "        )\n",
    "    ct_data = pro_gen_pd.copy()\n",
    "\n",
    "    # Fix potential length mismatches\n",
    "    if true_data.shape[0] > ct_data.shape[0]:\n",
    "        true_data = true_data[:ct_data.shape[0]]\n",
    "    elif true_data.shape[0] < ct_data.shape[0]:\n",
    "        ct_data = ct_data[:true_data.shape[0]]\n",
    "\n",
    "    # Evaluate\n",
    "    ct_auc = run_detection_metrics(\n",
    "        real=true_data, \n",
    "        synthetic=ct_data, \n",
    "    )['auc']\n",
    "\n",
    "    mmd = mmd_torch(synthetic=ct_data, real=true_data)\n",
    "\n",
    "    # Store\n",
    "    mmd_dict_ct[filename] = mmd\n",
    "    auc_dict_ct[filename] = ct_auc\n",
    "    data_dict_ct[filename] = ct_data.copy()\n",
    "\n",
    "\n",
    "    print(\"\"\"\\n ____________ Simulate w/ SDV ____________ \\n\"\"\")\n",
    "\n",
    "    true_data_sdv = true_data.copy()\n",
    "\n",
    "    # Creating same conditions as CausalTime\n",
    "    els = true_data_sdv.shape[0] % (true_data_sdv.shape[0]//20)\n",
    "    if els!=0:\n",
    "        true_data_sdv = true_data_sdv.loc[:-els, :]\n",
    "\n",
    "    # Sequence key\n",
    "    true_data_sdv.loc[:, 'id'] = [i for i in range(true_data_sdv.shape[0]//20) for _ in range(20)]\n",
    "\n",
    "    # Metadata\n",
    "    metadata = Metadata.detect_from_dataframe(data=true_data_sdv)\n",
    "    metadata.tables[\"table\"].columns[\"id\"][\"sdtype\"] = \"id\"\n",
    "    metadata.set_sequence_key(column_name='id')\n",
    "\n",
    "    # Synthesizer\n",
    "    synthesizer = PARSynthesizer(metadata)\n",
    "    synthesizer.fit(data=true_data_sdv)\n",
    "    synthetic_data = synthesizer.sample(num_sequences=true_data_sdv.shape[0]//20 + 1)\n",
    "\n",
    "    # Fix potential length mismatches\n",
    "    sdv_data = synthetic_data.loc[:len(true_data), :].drop(columns=[\"id\"])\n",
    "    if true_data.shape[0] > sdv_data.shape[0]:\n",
    "        true_data = true_data[:sdv_data.shape[0]]\n",
    "    elif true_data.shape[0] < sdv_data.shape[0]:\n",
    "        sdv_data = sdv_data[:true_data.shape[0]]\n",
    "    \n",
    "    mmd = mmd_torch(synthetic=sdv_data, real=true_data)\n",
    "\n",
    "    # Evaluate\n",
    "    sdv_auc = run_detection_metrics(\n",
    "        real=true_data, \n",
    "        synthetic=sdv_data, \n",
    "    )['auc']\n",
    "\n",
    "    # Store\n",
    "    mmd_dict_sdv[filename] = mmd\n",
    "    auc_dict_sdv[filename] = sdv_auc\n",
    "    data_dict_sdv[filename] = sdv_data.copy()\n",
    "\n",
    "\n",
    "    print(\"\"\"\\n _____________ Simulate w/ TimeVAE _____________ \\n\"\"\")\n",
    "    \n",
    "    # Prepare TimeVAE Data\n",
    "    dat = pd.read_csv(v[\"straight_path\"])\n",
    "\n",
    "    n_samples = dat.shape[0]\n",
    "    if 'target' in dat.columns:\n",
    "        X = dat.drop(columns=['target']) \n",
    "        y = dat['target'] \n",
    "    else:\n",
    "        X = dat\n",
    "        y = None\n",
    "\n",
    "    # # Synthcity TimeSeriesseems to require the time-series to be split per subject. We immitate this here. \n",
    "    # splits = np.array_split(X, 10)\n",
    "\n",
    "    temporal_data = [X]\n",
    "    observation_times = [X.index.to_numpy()]\n",
    "\n",
    "    # Initialize the TimeSeriesDataLoader\n",
    "    X_loader = TimeSeriesDataLoader(\n",
    "        temporal_data=temporal_data, \n",
    "        observation_times=observation_times, \n",
    "        outcome=y,\n",
    "        static_data=None,\n",
    "        train_size=1.0, \n",
    "        test_size=0.0\n",
    "    )\n",
    "\n",
    "    # Define plugin kwargs for TimeVAE\n",
    "    plugin_kwargs = dict(\n",
    "        n_iter=30,\n",
    "        batch_size=64,\n",
    "        lr=0.001,\n",
    "        encoder_n_layers_hidden=2,\n",
    "        decoder_n_layers_hidden=2,\n",
    "        encoder_dropout=0.05,\n",
    "        decoder_dropout=0.05\n",
    "    )\n",
    "\n",
    "    # Initialize the generative model for TimeVAE\n",
    "    test_plugin = Plugins().get(\"tvae\", **plugin_kwargs)\n",
    "    # test_plugin = Plugins().get(\"timegan\", ?)\n",
    "\n",
    "    # Fit the model\n",
    "    if y is not None:\n",
    "        test_plugin.fit(X_loader, cond=y)\n",
    "    else:\n",
    "        test_plugin.fit(X_loader)\n",
    "\n",
    "    # Generate synthetic data\n",
    "    generated_data = test_plugin.generate(count=n_samples) \n",
    "\n",
    "    # Extract the generated time-series data\n",
    "    generated_data = generated_data.data[\"seq_data\"]\n",
    "\n",
    "    # Drop unnecessary columns like \"seq_id\", \"seq_time_id\"\n",
    "    generated_data = generated_data.drop(columns=[\"seq_id\", \"seq_time_id\"])\n",
    "\n",
    "    # Fix potential length mismatches\n",
    "    if true_data.shape[0] > generated_data.shape[0]:\n",
    "        true_data = true_data[:generated_data.shape[0]]\n",
    "    elif true_data.shape[0] < generated_data.shape[0]:\n",
    "        generated_data = generated_data[:true_data.shape[0]]\n",
    "\n",
    "    mmd = mmd_torch(synthetic=generated_data, real=dat)\n",
    "\n",
    "    # Evaluate TimeVAE generated data\n",
    "    tvae_auc = sdv_auc = run_detection_metrics(\n",
    "        real=true_data, \n",
    "        synthetic=generated_data, \n",
    "    )['auc']\n",
    "\n",
    "    # Store results for TimeVAE\n",
    "    mmd_dict_tvae[filename] = mmd\n",
    "    auc_dict_tvae[filename] = tvae_auc\n",
    "    data_dict_tvae[filename] = generated_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCS Mean AUC: 0.7670037626410371\n",
      "TCS Var AUC: 0.020319226874602074\n",
      "TCS Mean MMD: 1.1623944615496338\n",
      "TCS Mean MMD: 0.1710423522980567\n",
      "CT Mean AUC: 1.0\n",
      "CT Var AUC: 0.0\n",
      "CT Mean MMD: 0.6491333236143405\n",
      "CT Mean MMD: 0.0004844083183019764\n",
      "CPAR Mean AUC: 1.0\n",
      "CPAR Var AUC: 0.0\n",
      "CPAR Mean MMD: 0.5265286248436134\n",
      "CPAR Var MMD: 0.001191348643831709\n",
      "TimeVAE Mean AUC: 1.0\n",
      "TimeVAE Var AUC: 0.0\n",
      "TimeVAE Mean MMD: 0.09763354326828\n",
      "TimeVAE Var MMD: 0.00016653213360517854\n"
     ]
    }
   ],
   "source": [
    "print(f\"TCS Mean AUC: {np.array(list(auc_dict_tcs.values())).mean()}\")\n",
    "print(f\"TCS Var AUC: {np.array(list(auc_dict_tcs.values())).var()}\")\n",
    "\n",
    "print(f\"TCS Mean MMD: {np.array(list(mmd_dict_tcs.values())).mean()}\")\n",
    "print(f\"TCS Mean MMD: {np.array(list(mmd_dict_tcs.values())).var()}\")\n",
    "\n",
    "print(f\"CT Mean AUC: {np.array(list(auc_dict_ct.values())).mean()}\")\n",
    "print(f\"CT Var AUC: {np.array(list(auc_dict_ct.values())).var()}\")\n",
    "\n",
    "print(f\"CT Mean MMD: {np.array(list(mmd_dict_ct.values())).mean()}\")\n",
    "print(f\"CT Mean MMD: {np.array(list(mmd_dict_ct.values())).var()}\")\n",
    "\n",
    "print(f\"CPAR Mean AUC: {np.array(list(auc_dict_sdv.values())).mean()}\")\n",
    "print(f\"CPAR Var AUC: {np.array(list(auc_dict_sdv.values())).var()}\")\n",
    "\n",
    "print(f\"CPAR Mean MMD: {np.array(list(mmd_dict_sdv.values())).mean()}\")\n",
    "print(f\"CPAR Var MMD: {np.array(list(mmd_dict_sdv.values())).var()}\")\n",
    "\n",
    "print(f\"TimeVAE Mean AUC: {np.array(list(auc_dict_tvae.values())).mean()}\")\n",
    "print(f\"TimeVAE Var AUC: {np.array(list(auc_dict_tvae.values())).var()}\")\n",
    "\n",
    "print(f\"TimeVAE Mean MMD: {np.array(list(mmd_dict_tvae.values())).mean()}\")\n",
    "print(f\"TimeVAE Var MMD: {np.array(list(mmd_dict_tvae.values())).var()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat\n",
    "save_path = Path(os.getcwd()).parents[1] / \"data\" / \"results\" / \"vs\"\n",
    "\n",
    "auc_dict_tcs\n",
    "auc_dict = {}\n",
    "mmd_dict = {}\n",
    "\n",
    "for k in auc_dict_tcs.keys():\n",
    "    auc_dict[k] = {\n",
    "        \"tcs\" : auc_dict_tcs[k], \n",
    "        \"ct\" : auc_dict_ct[k],\n",
    "        \"cpar\" : auc_dict_sdv[k],\n",
    "        \"tvae\" : auc_dict_tvae[k],\n",
    "    }\n",
    "\n",
    "for k in mmd_dict_tcs.keys():\n",
    "    mmd_dict[k] = {\n",
    "        \"tcs\" : mmd_dict_tcs[k], \n",
    "        \"ct\" : mmd_dict_ct[k],\n",
    "        \"cpar\" : mmd_dict_sdv[k],\n",
    "        \"tvae\" : mmd_dict_tvae[k],\n",
    "    }\n",
    "\n",
    "# # Save in JSON \n",
    "# json.dump(auc_dict, open(save_path / \"fmri_19_20_auc.json\", \"w\"))\n",
    "# json.dump(mmd_dict, open(save_path / \"fmri_19_20_mmd.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
